* 
* ==> Audit <==
* |---------|-------------------------------|----------|---------|---------|--------------------------------|--------------------------------|
| Command |             Args              | Profile  |  User   | Version |           Start Time           |            End Time            |
|---------|-------------------------------|----------|---------|---------|--------------------------------|--------------------------------|
| start   | --driver=docker               | minikube | brandon | v1.25.2 | Tue, 19 Apr 2022 09:33:27 CEST | Tue, 19 Apr 2022 09:39:48 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Tue, 19 Apr 2022 13:49:43 CEST | Tue, 19 Apr 2022 13:50:12 CEST |
| tunnel  |                               | minikube | brandon | v1.25.2 | Tue, 19 Apr 2022 14:18:22 CEST | Tue, 19 Apr 2022 16:06:42 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Tue, 19 Apr 2022 16:21:03 CEST | Tue, 19 Apr 2022 16:21:13 CEST |
| tunnel  |                               | minikube | brandon | v1.25.2 | Wed, 20 Apr 2022 09:18:02 CEST | Wed, 20 Apr 2022 14:04:50 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Wed, 20 Apr 2022 14:27:29 CEST | Wed, 20 Apr 2022 14:28:02 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Wed, 20 Apr 2022 14:45:49 CEST | Wed, 20 Apr 2022 14:46:25 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Wed, 20 Apr 2022 15:01:00 CEST | Wed, 20 Apr 2022 15:01:39 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Wed, 20 Apr 2022 15:08:29 CEST | Wed, 20 Apr 2022 15:08:59 CEST |
| addons  | disable ingress               | minikube | brandon | v1.25.2 | Thu, 18 Aug 2022 21:45:44 CEST | Thu, 18 Aug 2022 21:45:44 CEST |
| addons  | disable ingress               | minikube | brandon | v1.25.2 | Thu, 18 Aug 2022 21:50:35 CEST | Thu, 18 Aug 2022 21:50:35 CEST |
| start   | --driver docker -p docker     | docker   | brandon | v1.25.2 | Thu, 18 Aug 2022 21:50:08 CEST | Thu, 18 Aug 2022 21:50:51 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Thu, 18 Aug 2022 21:51:32 CEST | Thu, 18 Aug 2022 21:52:18 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Fri, 16 Sep 2022 09:32:50 CEST | Fri, 16 Sep 2022 09:33:26 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Fri, 16 Sep 2022 09:58:42 CEST | Fri, 16 Sep 2022 09:59:52 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Fri, 16 Sep 2022 13:01:47 CEST | Fri, 16 Sep 2022 13:02:53 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Fri, 16 Sep 2022 18:35:23 CEST | Fri, 16 Sep 2022 18:35:54 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:16:46 CEST | Sat, 17 Sep 2022 23:17:16 CEST |
| start   | --memory=4096 --driver=docker | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:36:47 CEST | Sat, 17 Sep 2022 23:37:19 CEST |
| kubectl | -- get pods -A                | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:37:25 CEST | Sat, 17 Sep 2022 23:37:27 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:51:50 CEST | Sat, 17 Sep 2022 23:52:25 CEST |
| kubectl | -- get pods -A                | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:52:34 CEST | Sat, 17 Sep 2022 23:52:36 CEST |
| stop    |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:52:49 CEST | Sat, 17 Sep 2022 23:53:07 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:53:21 CEST | Sat, 17 Sep 2022 23:53:51 CEST |
| kubectl | -- get pods -A                | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:54:05 CEST | Sat, 17 Sep 2022 23:54:08 CEST |
| stop    |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:56:39 CEST | Sat, 17 Sep 2022 23:56:56 CEST |
| start   |                               | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:57:06 CEST | Sat, 17 Sep 2022 23:57:39 CEST |
| kubectl | -- get pods -A                | minikube | brandon | v1.25.2 | Sat, 17 Sep 2022 23:57:55 CEST | Sat, 17 Sep 2022 23:57:57 CEST |
| stop    |                               | minikube | brandon | v1.27.0 | 18 Sep 22 00:02 CEST           | 18 Sep 22 00:02 CEST           |
| start   |                               | minikube | brandon | v1.27.0 | 18 Sep 22 00:02 CEST           | 18 Sep 22 00:03 CEST           |
| addons  | enable ingres                 | minikube | brandon | v1.27.0 | 18 Sep 22 00:03 CEST           |                                |
|---------|-------------------------------|----------|---------|---------|--------------------------------|--------------------------------|

* 
* ==> Dernier dÃ©marrage <==
* Log file created at: 2022/09/18 00:02:38
Running on machine: MBP-de-Brandon
Binary: Built with gc go1.19.1 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0918 00:02:38.666276   14118 out.go:296] Setting OutFile to fd 1 ...
I0918 00:02:38.666566   14118 out.go:348] isatty.IsTerminal(1) = true
I0918 00:02:38.666570   14118 out.go:309] Setting ErrFile to fd 2...
I0918 00:02:38.666578   14118 out.go:348] isatty.IsTerminal(2) = true
I0918 00:02:38.666776   14118 root.go:333] Updating PATH: /Users/brandon/.minikube/bin
W0918 00:02:38.666929   14118 root.go:310] Error reading config file at /Users/brandon/.minikube/config/config.json: open /Users/brandon/.minikube/config/config.json: no such file or directory
I0918 00:02:38.668748   14118 out.go:303] Setting JSON to false
I0918 00:02:38.698785   14118 start.go:115] hostinfo: {"hostname":"MBP-de-Brandon.lan","uptime":912,"bootTime":1663451246,"procs":465,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"11.6","kernelVersion":"20.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0c7ee418-203d-5941-a4d7-21acc16ccac7"}
W0918 00:02:38.698915   14118 start.go:123] gopshost.Virtualization returned error: not implemented yet
I0918 00:02:38.721647   14118 out.go:177] ðŸ˜„  minikube v1.27.0 sur Darwin 11.6
I0918 00:02:38.761517   14118 notify.go:214] Checking for updates...
I0918 00:02:38.761702   14118 preload.go:306] deleting older generation preload /Users/brandon/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4
I0918 00:02:38.762840   14118 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0918 00:02:38.784241   14118 out.go:177] ðŸ†•  Kubernetes 1.25.0 est dÃ©sormais disponible. Si vous souhaitez effectuer une mise Ã  niveau, spÃ©cifiez : --kubernetes-version=v1.25.0
I0918 00:02:38.784345   14118 preload.go:306] deleting older generation preload /Users/brandon/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4.checksum
I0918 00:02:38.803615   14118 driver.go:365] Setting default libvirt URI to qemu:///system
I0918 00:02:38.972428   14118 docker.go:137] docker version: linux-20.10.10
I0918 00:02:38.972566   14118 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 00:02:39.805663   14118 info.go:265] docker info: {ID:LWM6:DUOZ:LVJY:MI7X:CMWW:LNOV:ASKF:E33S:G5NF:GJ2J:DPDY:TFJK Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:47 SystemTime:2022-09-17 22:02:39.14029492 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4383428608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.10 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:5b46e404f6b9f661a205e28d59c982d3634148f8 Expected:5b46e404f6b9f661a205e28d59c982d3634148f8} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.1.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:0.9.0]] Warnings:<nil>}}
I0918 00:02:39.846355   14118 out.go:177] âœ¨  Utilisation du pilote docker basÃ© sur le profil existant
I0918 00:02:39.871035   14118 start.go:284] selected driver: docker
I0918 00:02:39.871067   14118 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.1.1@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0918 00:02:39.871276   14118 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0918 00:02:39.871687   14118 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 00:02:40.236735   14118 info.go:265] docker info: {ID:LWM6:DUOZ:LVJY:MI7X:CMWW:LNOV:ASKF:E33S:G5NF:GJ2J:DPDY:TFJK Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:47 SystemTime:2022-09-17 22:02:40.130244698 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4383428608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.10 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:5b46e404f6b9f661a205e28d59c982d3634148f8 Expected:5b46e404f6b9f661a205e28d59c982d3634148f8} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.1.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:0.9.0]] Warnings:<nil>}}
I0918 00:02:40.237108   14118 cni.go:95] Creating CNI manager for ""
I0918 00:02:40.237121   14118 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 00:02:40.237132   14118 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.1.1@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0918 00:02:40.275063   14118 out.go:177] ðŸ‘  DÃ©marrage du noeud de plan de contrÃ´le minikube dans le cluster minikube
I0918 00:02:40.294086   14118 cache.go:120] Beginning downloading kic base image for docker with docker
I0918 00:02:40.313818   14118 out.go:177] ðŸšœ  Extraction de l'image de base...
I0918 00:02:40.334530   14118 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0918 00:02:40.334623   14118 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon
I0918 00:02:40.494181   14118 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.23.3/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4
I0918 00:02:40.494200   14118 cache.go:57] Caching tarball of preloaded images
I0918 00:02:40.494579   14118 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0918 00:02:40.513071   14118 out.go:177] ðŸ’¾  TÃ©lÃ©chargement du prÃ©chargement de Kubernetes v1.23.3...
I0918 00:02:40.551336   14118 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4 ...
I0918 00:02:40.626758   14118 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon, skipping pull
I0918 00:02:40.626778   14118 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 exists in daemon, skipping load
I0918 00:02:40.746687   14118 download.go:101] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.23.3/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4?checksum=md5:2f70fd684ab4cdd2163dd5962aadfd10 -> /Users/brandon/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4
I0918 00:02:53.555946   14118 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4 ...
I0918 00:02:53.556119   14118 preload.go:256] verifying checksum of /Users/brandon/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-amd64.tar.lz4 ...
I0918 00:02:54.915568   14118 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.3 on docker
I0918 00:02:54.915829   14118 profile.go:148] Saving config to /Users/brandon/.minikube/profiles/minikube/config.json ...
I0918 00:02:54.916396   14118 cache.go:208] Successfully downloaded all kic artifacts
I0918 00:02:54.916435   14118 start.go:364] acquiring machines lock for minikube: {Name:mka5648ce879ddaeb0d6f736f93048f3fa77f9f7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0918 00:02:54.916571   14118 start.go:368] acquired machines lock for "minikube" in 92.732Âµs
I0918 00:02:54.916590   14118 start.go:96] Skipping create...Using existing machine configuration
I0918 00:02:54.916601   14118 fix.go:55] fixHost starting: 
I0918 00:02:54.917232   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:02:55.310798   14118 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0918 00:02:55.310849   14118 fix.go:129] unexpected machine state, will restart: <nil>
I0918 00:02:55.333866   14118 out.go:177] ðŸ”„  RedÃ©marrage du docker container existant pour "minikube" ...
I0918 00:02:55.370329   14118 cli_runner.go:164] Run: docker start minikube
I0918 00:02:56.511912   14118 cli_runner.go:217] Completed: docker start minikube: (1.141474299s)
I0918 00:02:56.512109   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:02:56.883430   14118 kic.go:415] container "minikube" state is running.
I0918 00:02:56.885672   14118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:02:57.345800   14118 profile.go:148] Saving config to /Users/brandon/.minikube/profiles/minikube/config.json ...
I0918 00:02:57.366333   14118 machine.go:88] provisioning docker machine ...
I0918 00:02:57.366376   14118 ubuntu.go:169] provisioning hostname "minikube"
I0918 00:02:57.366456   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:02:57.555424   14118 main.go:134] libmachine: Using SSH client type: native
I0918 00:02:57.556023   14118 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ebfe0] 0x43ef160 <nil>  [] 0s} 127.0.0.1 51680 <nil> <nil>}
I0918 00:02:57.556062   14118 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0918 00:02:57.559868   14118 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0918 00:03:00.744061   14118 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0918 00:03:00.744200   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:00.934962   14118 main.go:134] libmachine: Using SSH client type: native
I0918 00:03:00.935208   14118 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ebfe0] 0x43ef160 <nil>  [] 0s} 127.0.0.1 51680 <nil> <nil>}
I0918 00:03:00.935223   14118 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0918 00:03:01.072732   14118 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0918 00:03:01.072788   14118 ubuntu.go:175] set auth options {CertDir:/Users/brandon/.minikube CaCertPath:/Users/brandon/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/brandon/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/brandon/.minikube/machines/server.pem ServerKeyPath:/Users/brandon/.minikube/machines/server-key.pem ClientKeyPath:/Users/brandon/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/brandon/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/brandon/.minikube}
I0918 00:03:01.072809   14118 ubuntu.go:177] setting up certificates
I0918 00:03:01.072821   14118 provision.go:83] configureAuth start
I0918 00:03:01.072970   14118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:03:01.249772   14118 provision.go:138] copyHostCerts
I0918 00:03:01.249908   14118 exec_runner.go:144] found /Users/brandon/.minikube/key.pem, removing ...
I0918 00:03:01.249916   14118 exec_runner.go:207] rm: /Users/brandon/.minikube/key.pem
I0918 00:03:01.250100   14118 exec_runner.go:151] cp: /Users/brandon/.minikube/certs/key.pem --> /Users/brandon/.minikube/key.pem (1679 bytes)
I0918 00:03:01.250627   14118 exec_runner.go:144] found /Users/brandon/.minikube/ca.pem, removing ...
I0918 00:03:01.250632   14118 exec_runner.go:207] rm: /Users/brandon/.minikube/ca.pem
I0918 00:03:01.250740   14118 exec_runner.go:151] cp: /Users/brandon/.minikube/certs/ca.pem --> /Users/brandon/.minikube/ca.pem (1078 bytes)
I0918 00:03:01.251230   14118 exec_runner.go:144] found /Users/brandon/.minikube/cert.pem, removing ...
I0918 00:03:01.251237   14118 exec_runner.go:207] rm: /Users/brandon/.minikube/cert.pem
I0918 00:03:01.251329   14118 exec_runner.go:151] cp: /Users/brandon/.minikube/certs/cert.pem --> /Users/brandon/.minikube/cert.pem (1123 bytes)
I0918 00:03:01.251761   14118 provision.go:112] generating server cert: /Users/brandon/.minikube/machines/server.pem ca-key=/Users/brandon/.minikube/certs/ca.pem private-key=/Users/brandon/.minikube/certs/ca-key.pem org=brandon.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0918 00:03:01.338341   14118 provision.go:172] copyRemoteCerts
I0918 00:03:01.338415   14118 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0918 00:03:01.338464   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:01.688663   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:01.784566   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0918 00:03:01.814583   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0918 00:03:01.839839   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0918 00:03:01.865181   14118 provision.go:86] duration metric: configureAuth took 792.319808ms
I0918 00:03:01.865200   14118 ubuntu.go:193] setting minikube options for container-runtime
I0918 00:03:01.865609   14118 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0918 00:03:01.865723   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:02.072992   14118 main.go:134] libmachine: Using SSH client type: native
I0918 00:03:02.073238   14118 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ebfe0] 0x43ef160 <nil>  [] 0s} 127.0.0.1 51680 <nil> <nil>}
I0918 00:03:02.073247   14118 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0918 00:03:02.227363   14118 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0918 00:03:02.227372   14118 ubuntu.go:71] root file system type: overlay
I0918 00:03:02.227608   14118 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0918 00:03:02.227720   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:02.412160   14118 main.go:134] libmachine: Using SSH client type: native
I0918 00:03:02.412339   14118 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ebfe0] 0x43ef160 <nil>  [] 0s} 127.0.0.1 51680 <nil> <nil>}
I0918 00:03:02.412403   14118 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0918 00:03:02.560506   14118 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0918 00:03:02.560635   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:02.732525   14118 main.go:134] libmachine: Using SSH client type: native
I0918 00:03:02.732729   14118 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43ebfe0] 0x43ef160 <nil>  [] 0s} 127.0.0.1 51680 <nil> <nil>}
I0918 00:03:02.732741   14118 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0918 00:03:02.874401   14118 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0918 00:03:02.874416   14118 machine.go:91] provisioned docker machine in 5.507904963s
I0918 00:03:02.874442   14118 start.go:300] post-start starting for "minikube" (driver="docker")
I0918 00:03:02.874447   14118 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0918 00:03:02.874592   14118 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0918 00:03:02.874694   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:03.051207   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:03.153375   14118 ssh_runner.go:195] Run: cat /etc/os-release
I0918 00:03:03.158245   14118 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0918 00:03:03.158262   14118 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0918 00:03:03.158271   14118 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0918 00:03:03.158277   14118 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0918 00:03:03.158286   14118 filesync.go:126] Scanning /Users/brandon/.minikube/addons for local assets ...
I0918 00:03:03.158446   14118 filesync.go:126] Scanning /Users/brandon/.minikube/files for local assets ...
I0918 00:03:03.158520   14118 start.go:303] post-start completed in 284.062838ms
I0918 00:03:03.158644   14118 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0918 00:03:03.158693   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:03.332533   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:03.426718   14118 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0918 00:03:03.433387   14118 fix.go:57] fixHost completed within 8.516516732s
I0918 00:03:03.433407   14118 start.go:83] releasing machines lock for "minikube", held for 8.516565441s
I0918 00:03:03.433542   14118 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:03:03.612231   14118 ssh_runner.go:195] Run: systemctl --version
I0918 00:03:03.612310   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:03.613202   14118 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0918 00:03:03.613478   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:03.815048   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:03.827216   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:04.105989   14118 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0918 00:03:04.123191   14118 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0918 00:03:04.123270   14118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0918 00:03:04.141108   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0918 00:03:04.165629   14118 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0918 00:03:04.329911   14118 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0918 00:03:04.428607   14118 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 00:03:04.543901   14118 ssh_runner.go:195] Run: sudo systemctl restart docker
I0918 00:03:05.305044   14118 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 00:03:05.417902   14118 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 00:03:05.518031   14118 out.go:204] ðŸ³  PrÃ©paration de Kubernetes v1.23.3 sur Docker 20.10.12...
I0918 00:03:05.518166   14118 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0918 00:03:05.934897   14118 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0918 00:03:05.937158   14118 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0918 00:03:05.946109   14118 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 00:03:05.967641   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:03:06.493241   14118 out.go:177]     â–ª kubelet.housekeeping-interval=5m
I0918 00:03:06.514913   14118 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0918 00:03:06.515075   14118 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 00:03:06.702770   14118 docker.go:611] Got preloaded images: -- stdout --
busybox:latest
mariadb:latest
<none>:<none>
wurstmeister/kafka:latest
mysql:8.0.28
origindocker13/client:latest
<none>:<none>
<none>:<none>
jhipster/jhipster-registry:v7.3.0
grafana/grafana:8.4.1
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
<none>:<none>
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5
quay.io/coreos/prometheus-operator:v0.42.1
wurstmeister/zookeeper:latest
k8s.gcr.io/echoserver:1.4

-- /stdout --
I0918 00:03:06.703120   14118 docker.go:542] Images already preloaded, skipping extraction
I0918 00:03:06.705759   14118 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 00:03:06.832834   14118 docker.go:611] Got preloaded images: -- stdout --
busybox:latest
mariadb:latest
<none>:<none>
wurstmeister/kafka:latest
mysql:8.0.28
origindocker13/client:latest
<none>:<none>
<none>:<none>
jhipster/jhipster-registry:v7.3.0
grafana/grafana:8.4.1
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
<none>:<none>
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5
quay.io/coreos/prometheus-operator:v0.42.1
wurstmeister/zookeeper:latest
k8s.gcr.io/echoserver:1.4

-- /stdout --
I0918 00:03:06.832873   14118 cache_images.go:84] Images are preloaded, skipping loading
I0918 00:03:06.833116   14118 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0918 00:03:07.544961   14118 cni.go:95] Creating CNI manager for ""
I0918 00:03:07.544997   14118 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 00:03:07.545059   14118 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0918 00:03:07.545102   14118 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.23.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I0918 00:03:07.545606   14118 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0918 00:03:07.545939   14118 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0918 00:03:07.546076   14118 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.3
I0918 00:03:07.569147   14118 binaries.go:44] Found k8s binaries, skipping transfer
I0918 00:03:07.569324   14118 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0918 00:03:07.588247   14118 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (361 bytes)
I0918 00:03:07.622364   14118 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0918 00:03:07.660087   14118 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I0918 00:03:07.698067   14118 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0918 00:03:07.709050   14118 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 00:03:07.732759   14118 certs.go:54] Setting up /Users/brandon/.minikube/profiles/minikube for IP: 192.168.49.2
I0918 00:03:07.733390   14118 certs.go:182] skipping minikubeCA CA generation: /Users/brandon/.minikube/ca.key
I0918 00:03:07.734033   14118 certs.go:182] skipping proxyClientCA CA generation: /Users/brandon/.minikube/proxy-client-ca.key
I0918 00:03:07.735552   14118 certs.go:298] skipping minikube-user signed cert generation: /Users/brandon/.minikube/profiles/minikube/client.key
I0918 00:03:07.736376   14118 certs.go:298] skipping minikube signed cert generation: /Users/brandon/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0918 00:03:07.737110   14118 certs.go:298] skipping aggregator signed cert generation: /Users/brandon/.minikube/profiles/minikube/proxy-client.key
I0918 00:03:07.738632   14118 certs.go:388] found cert: /Users/brandon/.minikube/certs/Users/brandon/.minikube/certs/ca-key.pem (1675 bytes)
I0918 00:03:07.739068   14118 certs.go:388] found cert: /Users/brandon/.minikube/certs/Users/brandon/.minikube/certs/ca.pem (1078 bytes)
I0918 00:03:07.739177   14118 certs.go:388] found cert: /Users/brandon/.minikube/certs/Users/brandon/.minikube/certs/cert.pem (1123 bytes)
I0918 00:03:07.739728   14118 certs.go:388] found cert: /Users/brandon/.minikube/certs/Users/brandon/.minikube/certs/key.pem (1679 bytes)
I0918 00:03:07.742014   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0918 00:03:07.789241   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0918 00:03:07.845164   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0918 00:03:07.903670   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0918 00:03:07.956504   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0918 00:03:08.007480   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0918 00:03:08.057151   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0918 00:03:08.108896   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0918 00:03:08.168612   14118 ssh_runner.go:362] scp /Users/brandon/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0918 00:03:08.208347   14118 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0918 00:03:08.245449   14118 ssh_runner.go:195] Run: openssl version
I0918 00:03:08.264566   14118 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0918 00:03:08.286075   14118 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0918 00:03:08.299227   14118 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Apr 19 07:39 /usr/share/ca-certificates/minikubeCA.pem
I0918 00:03:08.299451   14118 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0918 00:03:08.317551   14118 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0918 00:03:08.347350   14118 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.1.1@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0918 00:03:08.349517   14118 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0918 00:03:08.497099   14118 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0918 00:03:08.527838   14118 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I0918 00:03:08.527874   14118 kubeadm.go:627] restartCluster start
I0918 00:03:08.528060   14118 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0918 00:03:08.554324   14118 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0918 00:03:08.555501   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:03:09.187642   14118 kubeconfig.go:116] verify returned: extract IP: "minikube" does not appear in /Users/brandon/.kube/config
I0918 00:03:09.188715   14118 kubeconfig.go:127] "minikube" context is missing from /Users/brandon/.kube/config - will repair!
I0918 00:03:09.190205   14118 lock.go:35] WriteFile acquiring /Users/brandon/.kube/config: {Name:mke34f3ab13b09d7d2a0235dce7cda0509b80154 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:03:09.193807   14118 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0918 00:03:09.226409   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:09.226738   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:09.283153   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:09.487520   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:09.487671   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:09.535441   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:09.686389   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:09.686503   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:09.727311   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:09.886017   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:09.886342   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:09.950835   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:10.085904   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:10.086011   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:10.128672   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:10.283372   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:10.283871   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:10.341134   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:10.486825   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:10.487083   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:10.527936   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:10.683597   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:10.683790   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:10.723111   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:10.887957   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:10.888401   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:10.949232   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:11.087851   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:11.088333   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:11.168721   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:11.286410   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:11.287080   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:11.342981   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:11.487767   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:11.489353   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:11.544816   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:11.684048   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:11.684239   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:11.747249   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:11.936680   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:11.938777   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:12.027114   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:12.086177   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:12.086587   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:12.134077   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:12.287513   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:12.287664   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:12.383779   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:12.383791   14118 api_server.go:165] Checking apiserver status ...
I0918 00:03:12.383921   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:03:12.437297   14118 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:03:12.437334   14118 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I0918 00:03:12.437697   14118 kubeadm.go:1114] stopping kube-system containers ...
I0918 00:03:12.438449   14118 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0918 00:03:12.640510   14118 docker.go:443] Stopping containers: [d5c1a70b8b0f 5f453d31dc55 32fb2b4ca27e c156e58a73a1 3290c092b3d4 c91557be79d1 7a948cf1dff1 91503d3b686a 6bebc73c58a5 22d29df32811 5177b031a40a 1e980c4a22d6 149e376f915d 2f8ada8a74e5 0252c30f52b5 ff28a6e376d5 ee967fb84047 1d8438785bb6 e9249adf1a2e 3b276f2dc3ee 0749c26c50f1 7ef021774feb 4d4c61223651 7473df53562a 527d76679455 0344b00e85b8 4680b910e981]
I0918 00:03:12.640845   14118 ssh_runner.go:195] Run: docker stop d5c1a70b8b0f 5f453d31dc55 32fb2b4ca27e c156e58a73a1 3290c092b3d4 c91557be79d1 7a948cf1dff1 91503d3b686a 6bebc73c58a5 22d29df32811 5177b031a40a 1e980c4a22d6 149e376f915d 2f8ada8a74e5 0252c30f52b5 ff28a6e376d5 ee967fb84047 1d8438785bb6 e9249adf1a2e 3b276f2dc3ee 0749c26c50f1 7ef021774feb 4d4c61223651 7473df53562a 527d76679455 0344b00e85b8 4680b910e981
I0918 00:03:12.756207   14118 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0918 00:03:12.797612   14118 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0918 00:03:12.841751   14118 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Sep 16 11:02 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Sep 17 21:57 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Sep 16 11:02 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Sep 17 21:57 /etc/kubernetes/scheduler.conf

I0918 00:03:12.842280   14118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0918 00:03:12.910328   14118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0918 00:03:12.948264   14118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0918 00:03:12.978503   14118 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0918 00:03:12.979123   14118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0918 00:03:13.017518   14118 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0918 00:03:13.059522   14118 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0918 00:03:13.059910   14118 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0918 00:03:13.094984   14118 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0918 00:03:13.126593   14118 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0918 00:03:13.126646   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:13.643798   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:17.130734   14118 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (3.486677162s)
I0918 00:03:17.130927   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:17.483446   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:17.582042   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:17.657230   14118 api_server.go:51] waiting for apiserver process to appear ...
I0918 00:03:17.657468   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:18.192658   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:18.683539   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:19.184912   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:19.686281   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:20.185968   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:20.683527   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:21.186548   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:21.682832   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:22.184709   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:22.243368   14118 api_server.go:71] duration metric: took 4.586006364s to wait for apiserver process to appear ...
I0918 00:03:22.243399   14118 api_server.go:87] waiting for apiserver healthz status ...
I0918 00:03:22.243413   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:22.247086   14118 api_server.go:256] stopped: https://127.0.0.1:51679/healthz: Get "https://127.0.0.1:51679/healthz": EOF
I0918 00:03:22.750781   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:22.753237   14118 api_server.go:256] stopped: https://127.0.0.1:51679/healthz: Get "https://127.0.0.1:51679/healthz": EOF
I0918 00:03:23.251773   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:27.353054   14118 api_server.go:266] https://127.0.0.1:51679/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0918 00:03:27.353101   14118 api_server.go:102] status: https://127.0.0.1:51679/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0918 00:03:27.747645   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:27.788568   14118 api_server.go:266] https://127.0.0.1:51679/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0918 00:03:27.788676   14118 api_server.go:102] status: https://127.0.0.1:51679/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0918 00:03:28.252424   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:28.267550   14118 api_server.go:266] https://127.0.0.1:51679/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0918 00:03:28.267581   14118 api_server.go:102] status: https://127.0.0.1:51679/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0918 00:03:28.751127   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:28.761743   14118 api_server.go:266] https://127.0.0.1:51679/healthz returned 200:
ok
I0918 00:03:28.775352   14118 api_server.go:140] control plane version: v1.23.3
I0918 00:03:28.775373   14118 api_server.go:130] duration metric: took 6.53177268s to wait for apiserver health ...
I0918 00:03:28.775402   14118 cni.go:95] Creating CNI manager for ""
I0918 00:03:28.775425   14118 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 00:03:28.775442   14118 system_pods.go:43] waiting for kube-system pods to appear ...
I0918 00:03:28.792064   14118 system_pods.go:59] 7 kube-system pods found
I0918 00:03:28.792084   14118 system_pods.go:61] "coredns-64897985d-g8vpv" [0898f971-c3e4-48ec-980c-54e01eb2db16] Running
I0918 00:03:28.792087   14118 system_pods.go:61] "etcd-minikube" [26a4b1cf-8fd0-4b79-95f3-b844fabc261d] Running
I0918 00:03:28.792090   14118 system_pods.go:61] "kube-apiserver-minikube" [deb122e1-0647-4905-8178-caf71318061e] Running
I0918 00:03:28.792093   14118 system_pods.go:61] "kube-controller-manager-minikube" [aa9c95f7-2830-4326-a087-1c6e495e3335] Running
I0918 00:03:28.792096   14118 system_pods.go:61] "kube-proxy-kljks" [23e9557e-6c09-489e-96a7-27e88b443b88] Running
I0918 00:03:28.792099   14118 system_pods.go:61] "kube-scheduler-minikube" [5360f40b-3f66-448e-98b9-5788bd41e40c] Running
I0918 00:03:28.792101   14118 system_pods.go:61] "storage-provisioner" [abc3491c-5e34-4d70-8505-0a1d6285582e] Running
I0918 00:03:28.792105   14118 system_pods.go:74] duration metric: took 16.659192ms to wait for pod list to return data ...
I0918 00:03:28.792111   14118 node_conditions.go:102] verifying NodePressure condition ...
I0918 00:03:28.798962   14118 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0918 00:03:28.798977   14118 node_conditions.go:123] node cpu capacity is 4
I0918 00:03:28.799004   14118 node_conditions.go:105] duration metric: took 6.884718ms to run NodePressure ...
I0918 00:03:28.799032   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:03:29.021738   14118 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0918 00:03:29.043908   14118 ops.go:34] apiserver oom_adj: -16
I0918 00:03:29.043916   14118 kubeadm.go:631] restartCluster took 20.515415378s
I0918 00:03:29.043922   14118 kubeadm.go:398] StartCluster complete in 20.696058521s
I0918 00:03:29.043962   14118 settings.go:142] acquiring lock: {Name:mk43f0b5c75c4c6fc07655ad5b27752769f43c25 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:03:29.044124   14118 settings.go:150] Updating kubeconfig:  /Users/brandon/.kube/config
I0918 00:03:29.046584   14118 lock.go:35] WriteFile acquiring /Users/brandon/.kube/config: {Name:mke34f3ab13b09d7d2a0235dce7cda0509b80154 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:03:29.053055   14118 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0918 00:03:29.053140   14118 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0918 00:03:29.053218   14118 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0918 00:03:29.053275   14118 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0918 00:03:29.054235   14118 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0918 00:03:29.073515   14118 out.go:177] ðŸ”Ž  VÃ©rification des composants Kubernetes...
I0918 00:03:29.073596   14118 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0918 00:03:29.073597   14118 addons.go:65] Setting dashboard=true in profile "minikube"
I0918 00:03:29.073609   14118 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0918 00:03:29.113271   14118 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0918 00:03:29.113289   14118 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0918 00:03:29.113331   14118 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0918 00:03:29.113349   14118 addons.go:162] addon storage-provisioner should already be in state true
I0918 00:03:29.113402   14118 addons.go:153] Setting addon dashboard=true in "minikube"
W0918 00:03:29.113409   14118 addons.go:162] addon dashboard should already be in state true
I0918 00:03:29.113694   14118 host.go:66] Checking if "minikube" exists ...
I0918 00:03:29.113695   14118 host.go:66] Checking if "minikube" exists ...
I0918 00:03:29.114800   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:03:29.115138   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:03:29.115158   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:03:29.728723   14118 start.go:790] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0918 00:03:29.728733   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:03:29.896340   14118 out.go:177]     â–ª Utilisation de l'image docker.io/kubernetesui/dashboard:v2.6.0
I0918 00:03:29.879774   14118 out.go:177]     â–ª Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I0918 00:03:29.886752   14118 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0918 00:03:29.896376   14118 addons.go:162] addon default-storageclass should already be in state true
I0918 00:03:29.913199   14118 host.go:66] Checking if "minikube" exists ...
I0918 00:03:29.929379   14118 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:03:29.946076   14118 out.go:177]     â–ª Utilisation de l'image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0918 00:03:29.930501   14118 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0918 00:03:29.946131   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0918 00:03:29.946287   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:29.961464   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0918 00:03:29.961494   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0918 00:03:29.962507   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:30.080500   14118 api_server.go:51] waiting for apiserver process to appear ...
I0918 00:03:30.080594   14118 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:03:30.124311   14118 api_server.go:71] duration metric: took 1.071007251s to wait for apiserver process to appear ...
I0918 00:03:30.124334   14118 api_server.go:87] waiting for apiserver healthz status ...
I0918 00:03:30.124357   14118 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:51679/healthz ...
I0918 00:03:30.140680   14118 api_server.go:266] https://127.0.0.1:51679/healthz returned 200:
ok
I0918 00:03:30.144416   14118 api_server.go:140] control plane version: v1.23.3
I0918 00:03:30.144442   14118 api_server.go:130] duration metric: took 20.101014ms to wait for apiserver health ...
I0918 00:03:30.144450   14118 system_pods.go:43] waiting for kube-system pods to appear ...
I0918 00:03:30.159146   14118 system_pods.go:59] 7 kube-system pods found
I0918 00:03:30.159162   14118 system_pods.go:61] "coredns-64897985d-g8vpv" [0898f971-c3e4-48ec-980c-54e01eb2db16] Running
I0918 00:03:30.159167   14118 system_pods.go:61] "etcd-minikube" [26a4b1cf-8fd0-4b79-95f3-b844fabc261d] Running
I0918 00:03:30.159172   14118 system_pods.go:61] "kube-apiserver-minikube" [deb122e1-0647-4905-8178-caf71318061e] Running
I0918 00:03:30.159177   14118 system_pods.go:61] "kube-controller-manager-minikube" [aa9c95f7-2830-4326-a087-1c6e495e3335] Running
I0918 00:03:30.159182   14118 system_pods.go:61] "kube-proxy-kljks" [23e9557e-6c09-489e-96a7-27e88b443b88] Running
I0918 00:03:30.159186   14118 system_pods.go:61] "kube-scheduler-minikube" [5360f40b-3f66-448e-98b9-5788bd41e40c] Running
I0918 00:03:30.159190   14118 system_pods.go:61] "storage-provisioner" [abc3491c-5e34-4d70-8505-0a1d6285582e] Running
I0918 00:03:30.159195   14118 system_pods.go:74] duration metric: took 14.740261ms to wait for pod list to return data ...
I0918 00:03:30.159203   14118 kubeadm.go:573] duration metric: took 1.105904349s to wait for : map[apiserver:true system_pods:true] ...
I0918 00:03:30.159213   14118 node_conditions.go:102] verifying NodePressure condition ...
I0918 00:03:30.166005   14118 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0918 00:03:30.166022   14118 node_conditions.go:123] node cpu capacity is 4
I0918 00:03:30.166031   14118 node_conditions.go:105] duration metric: took 6.814048ms to run NodePressure ...
I0918 00:03:30.166043   14118 start.go:216] waiting for startup goroutines ...
I0918 00:03:30.253870   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:30.253997   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:30.254217   14118 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0918 00:03:30.254223   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0918 00:03:30.254310   14118 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:03:30.441812   14118 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0918 00:03:30.465169   14118 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51680 SSHKeyPath:/Users/brandon/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:03:30.486021   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0918 00:03:30.486030   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0918 00:03:30.590684   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0918 00:03:30.590705   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0918 00:03:30.706159   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0918 00:03:30.706171   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0918 00:03:30.773889   14118 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0918 00:03:30.982133   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0918 00:03:30.982152   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4298 bytes)
I0918 00:03:31.121382   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-role.yaml
I0918 00:03:31.121392   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0918 00:03:31.313446   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0918 00:03:31.313459   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0918 00:03:31.485592   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0918 00:03:31.485654   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0918 00:03:31.713059   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0918 00:03:31.713075   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0918 00:03:32.079120   14118 addons.go:345] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0918 00:03:32.079144   14118 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0918 00:03:32.272071   14118 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0918 00:03:34.003781   14118 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.561807784s)
I0918 00:03:34.003831   14118 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.229825942s)
I0918 00:03:34.886028   14118 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.613763728s)
I0918 00:03:34.928266   14118 out.go:177] ðŸŒŸ  Modules activÃ©s: storage-provisioner, default-storageclass, dashboard
I0918 00:03:34.993447   14118 addons.go:414] enableAddons completed in 5.940021225s
I0918 00:03:36.751747   14118 start.go:506] kubectl: 1.21.5, cluster: 1.23.3 (minor skew: 2)
I0918 00:03:36.780042   14118 out.go:177] 
W0918 00:03:36.810702   14118 out.go:239] â—  /usr/local/bin/kubectl est la version 1.21.5, qui peut comporter des incompatibilitÃ©s avec Kubernetes 1.23.3.
I0918 00:03:36.843049   14118 out.go:177]     â–ª Vous voulez kubectl v1.23.3Â ? Essayez 'minikube kubectl -- get pods -A'
I0918 00:03:36.982413   14118 out.go:177] ðŸ„  TerminÃ© ! kubectl est maintenant configurÃ© pour utiliser "minikube" cluster et espace de noms "default" par dÃ©faut.

* 
* ==> Docker <==
* -- Logs begin at Sat 2022-09-17 22:02:57 UTC, end at Sat 2022-09-17 22:04:25 UTC. --
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.148672132Z" level=info msg="Starting up"
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.156611208Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.156660332Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.156683169Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.156704460Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.160662866Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.160704434Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.160723061Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.160730437Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.178941597Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.500207732Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.500255750Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.500976208Z" level=info msg="Loading containers: start."
Sep 17 22:02:58 minikube dockerd[213]: time="2022-09-17T22:02:58.907159793Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 17 22:02:59 minikube dockerd[213]: time="2022-09-17T22:02:59.015967712Z" level=info msg="Loading containers: done."
Sep 17 22:02:59 minikube dockerd[213]: time="2022-09-17T22:02:59.059665549Z" level=info msg="Docker daemon" commit=459d0df graphdriver(s)=overlay2 version=20.10.12
Sep 17 22:02:59 minikube dockerd[213]: time="2022-09-17T22:02:59.060190038Z" level=info msg="Daemon has completed initialization"
Sep 17 22:02:59 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 22:02:59 minikube dockerd[213]: time="2022-09-17T22:02:59.141274866Z" level=info msg="API listen on [::]:2376"
Sep 17 22:02:59 minikube dockerd[213]: time="2022-09-17T22:02:59.145539836Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 22:03:04 minikube systemd[1]: Stopping Docker Application Container Engine...
Sep 17 22:03:04 minikube dockerd[213]: time="2022-09-17T22:03:04.576246850Z" level=info msg="Processing signal 'terminated'"
Sep 17 22:03:04 minikube dockerd[213]: time="2022-09-17T22:03:04.579887989Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Sep 17 22:03:04 minikube dockerd[213]: time="2022-09-17T22:03:04.581466169Z" level=info msg="Daemon shutdown complete"
Sep 17 22:03:04 minikube systemd[1]: docker.service: Succeeded.
Sep 17 22:03:04 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 17 22:03:04 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.681916450Z" level=info msg="Starting up"
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.685456026Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.685568498Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.685668567Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.685772326Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.687913256Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.688118579Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.688263272Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.688337618Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.696857428Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.726190147Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.726251948Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Sep 17 22:03:04 minikube dockerd[504]: time="2022-09-17T22:03:04.726579756Z" level=info msg="Loading containers: start."
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.160663377Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.247118653Z" level=info msg="Loading containers: done."
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.280386773Z" level=info msg="Docker daemon" commit=459d0df graphdriver(s)=overlay2 version=20.10.12
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.280660275Z" level=info msg="Daemon has completed initialization"
Sep 17 22:03:05 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.321354957Z" level=info msg="API listen on [::]:2376"
Sep 17 22:03:05 minikube dockerd[504]: time="2022-09-17T22:03:05.326825956Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 22:03:58 minikube dockerd[504]: time="2022-09-17T22:03:58.987098245Z" level=info msg="ignoring event" container=93ff39e50aeb99c4064cec6185fec7cdde516539df8277049a37357895e2320c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:03:59 minikube dockerd[504]: time="2022-09-17T22:03:59.561364863Z" level=info msg="ignoring event" container=1c25cbe1d4810bb366a6cba68f91467e294585a3c3b54e4a40811a7f9bfbe996 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:01 minikube dockerd[504]: time="2022-09-17T22:04:01.562140207Z" level=info msg="ignoring event" container=63367c26902fa048e82d3d048923f0c58d69553f821da8fff0e5530c3d6128f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:02 minikube dockerd[504]: time="2022-09-17T22:04:02.485907559Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Sep 17 22:04:03 minikube dockerd[504]: time="2022-09-17T22:04:03.298862304Z" level=info msg="ignoring event" container=1a0747319479b0aca1e773d681177eac168f4e0db05f75927b90aa9a94a3c430 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:12 minikube dockerd[504]: time="2022-09-17T22:04:12.405583819Z" level=info msg="ignoring event" container=15b63c33e3547add8889e11c63a0e59157aa583405da0c2f3003503dae65a058 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:16 minikube dockerd[504]: time="2022-09-17T22:04:16.297773136Z" level=info msg="ignoring event" container=26c61b45899c030eb72d16b927ff8541da1c41512d3e704a2d15d0912c5f7241 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:16 minikube dockerd[504]: time="2022-09-17T22:04:16.480876511Z" level=info msg="ignoring event" container=685130cd7095327d38668bfdaa88a6a34bc0a1542f2570be07aaa92fd26e90db module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:17 minikube dockerd[504]: time="2022-09-17T22:04:17.259549505Z" level=warning msg="reference for unknown type: " digest="sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3" remote="docker.io/kubernetesui/dashboard@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3"
Sep 17 22:04:17 minikube dockerd[504]: time="2022-09-17T22:04:17.560325693Z" level=info msg="ignoring event" container=96a597c46b3122bf8697ef7f0766bf817ce660791e1b1ae56abb41d8a6f9aa6d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:19 minikube dockerd[504]: time="2022-09-17T22:04:19.571198327Z" level=info msg="ignoring event" container=8282a462cc3d754b38108bafa008282defb567e3b24c5c1e1b9e58ede4b8a5f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:19 minikube dockerd[504]: time="2022-09-17T22:04:19.874297463Z" level=info msg="ignoring event" container=2aa11da2fd6efe52b034e09096c198b9e2582fd4b6aaeb0d286771a699882624 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 22:04:24 minikube dockerd[504]: time="2022-09-17T22:04:24.178489319Z" level=info msg="ignoring event" container=9cb5c95f7b01922aa5e3107bf7ae2c1a8d7878c402629a25b573a8bd52335037 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED              STATE               NAME                        ATTEMPT             POD ID
237ca5a1cdf72       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   10 seconds ago       Running             dashboard-metrics-scraper   0                   b1bdb1783b0fc
1a0747319479b       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        24 seconds ago       Exited              init-mariadb                5                   7791170d19646
63367c26902fa       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        26 seconds ago       Exited              init-mariadb                2                   9b6a31ae03582
1c25cbe1d4810       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        28 seconds ago       Exited              init-mariadb                2                   cde560bb46f10
93ff39e50aeb9       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        30 seconds ago       Exited              init-mariadb                5                   15fc379ecc620
685130cd70953       busybox@sha256:ad9bd57a3a57cc95515c537b89aaa69d83a6df54c4050fcf2b41ad367bec0cd5                        31 seconds ago       Exited              init-ds                     13                  dc41e5969ba98
c0bdf1cd85971       busybox@sha256:ad9bd57a3a57cc95515c537b89aaa69d83a6df54c4050fcf2b41ad367bec0cd5                        33 seconds ago       Running             init-ds                     15                  d7a7c01e4bb5b
9cb5c95f7b019       busybox@sha256:ad9bd57a3a57cc95515c537b89aaa69d83a6df54c4050fcf2b41ad367bec0cd5                        35 seconds ago       Exited              init-ds                     14                  c03ae1bac2a10
08b7342c00e94       3f43f72cb2832                                                                                          38 seconds ago       Running             zookeeper                   6                   12f84ae68b9d3
5d92a2b3d561e       busybox@sha256:ad9bd57a3a57cc95515c537b89aaa69d83a6df54c4050fcf2b41ad367bec0cd5                        38 seconds ago       Running             init-ds                     4                   dcf15dbf68b1b
d1cbff5b5e6f6       f208c8cdbc543                                                                                          44 seconds ago       Running             jhipster-registry           21                  0482e411be7ee
a78a2e7d9b518       a692873757c06                                                                                          44 seconds ago       Running             kafka-broker                12                  48190e193000e
96a597c46b312       f208c8cdbc543                                                                                          47 seconds ago       Exited              jhipster-registry           20                  48e0d1f1308a5
ac3fbb6923b27       a90209bb39e3d                                                                                          47 seconds ago       Running             echoserver                  15                  e026914329dcf
03132c436f495       e1482a24335a6                                                                                          48 seconds ago       Running             kubernetes-dashboard        36                  90b2f8ca5e422
26c61b45899c0       072763fd75f37                                                                                          48 seconds ago       Exited              prometheus-operator         23                  e789766049e93
e48bbb895c591       f2ad9f23df82a                                                                                          49 seconds ago       Running             mysql                       7                   204c36c6c1c5f
2d9e008086a7a       a90209bb39e3d                                                                                          50 seconds ago       Running             echoserver                  15                  cce2bb3a94598
95d0c11b1b9df       f2ad9f23df82a                                                                                          51 seconds ago       Running             mysql                       19                  22b6cd63d5a23
3f27e9ac11c11       a4ca41631cc7a                                                                                          51 seconds ago       Running             coredns                     22                  762ce164ef41c
15b63c33e3547       6e38f40d628db                                                                                          51 seconds ago       Exited              storage-provisioner         36                  52edc8b77f2bc
1a76d49de4837       9b7cc99821098                                                                                          52 seconds ago       Running             kube-proxy                  16                  f6625f44aaf46
4410a9ec8c964       a90209bb39e3d                                                                                          54 seconds ago       Running             echoserver                  16                  07d2a1d86a8cb
ab48ee5a28194       66f33ae1931d0                                                                                          56 seconds ago       Running             jhipster-grafana            15                  733e4f2044168
2867bd0518d03       f40be0088a83e                                                                                          About a minute ago   Running             kube-apiserver              19                  7a8cc64edffc8
202ed50db9413       99a3486be4f28                                                                                          About a minute ago   Running             kube-scheduler              17                  a7b3f3a4ff584
3ad8ba9010806       25f8c7f3da61c                                                                                          About a minute ago   Running             etcd                        16                  d9a61be45455c
d779867ef9069       b07520cd7ab76                                                                                          About a minute ago   Running             kube-controller-manager     20                  f32b8859a0b6e
6201301ac7c18       origindocker13/client@sha256:198b2f0bad5910e8846940e264ea4ff4445695470f2480f8319c4e61665f7def          2 minutes ago        Exited              client-app                  22                  056aea8b5b312
c6ae4baf51153       origindocker13/client@sha256:198b2f0bad5910e8846940e264ea4ff4445695470f2480f8319c4e61665f7def          2 minutes ago        Exited              client-app                  20                  d4ddf3b44f961
5f731b3df75ce       a692873757c06                                                                                          3 minutes ago        Exited              kafka-broker                11                  2b8eab18b4eda
5969f5c58dc43       f208c8cdbc543                                                                                          3 minutes ago        Exited              jhipster-registry           20                  af0904819d99a
5f773ea36332a       f2ad9f23df82a                                                                                          3 minutes ago        Exited              mysql                       6                   89d964fabd0b6
c0042187072fc       f2ad9f23df82a                                                                                          3 minutes ago        Exited              mysql                       18                  643a236b684ca
bda219223c48d       e1482a24335a6                                                                                          5 minutes ago        Exited              kubernetes-dashboard        35                  93578e1ed3226
ee5b3538a8508       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        6 minutes ago        Exited              auth-database               1                   42abe9bf35068
b9152d23a1a9f       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        6 minutes ago        Exited              auth-database               2                   c56397e58c6ad
e94345235376f       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        6 minutes ago        Exited              order-database              2                   5452766430c84
87c437a30e714       mariadb@sha256:ca04948aca834499f728692520eff82917de1d768b47751ba4dd0fc5f261c8e7                        6 minutes ago        Exited              order-database              1                   c279c560df1b0
67e1029d1e705       3f43f72cb2832                                                                                          6 minutes ago        Exited              zookeeper                   5                   c673f0f5c85e2
1981c3e6f4e68       a90209bb39e3d                                                                                          6 minutes ago        Exited              echoserver                  14                  82d3be272e75f
a42c26eb3d957       a90209bb39e3d                                                                                          6 minutes ago        Exited              echoserver                  15                  af18ae07474c6
ff2c4c0057cff       66f33ae1931d0                                                                                          6 minutes ago        Exited              jhipster-grafana            14                  1ad159de0e62c
32fb2b4ca27e0       9b7cc99821098                                                                                          6 minutes ago        Exited              kube-proxy                  15                  c156e58a73a1d
f63c8a90dc8f7       a90209bb39e3d                                                                                          6 minutes ago        Exited              echoserver                  14                  6be08908df60c
c91557be79d11       a4ca41631cc7a                                                                                          6 minutes ago        Exited              coredns                     21                  7a948cf1dff16
91503d3b686a6       b07520cd7ab76                                                                                          6 minutes ago        Exited              kube-controller-manager     19                  149e376f915d6
6bebc73c58a5e       f40be0088a83e                                                                                          6 minutes ago        Exited              kube-apiserver              18                  0252c30f52b5d
22d29df328110       25f8c7f3da61c                                                                                          6 minutes ago        Exited              etcd                        15                  1e980c4a22d6f
5177b031a40a9       99a3486be4f28                                                                                          6 minutes ago        Exited              kube-scheduler              16                  2f8ada8a74e59

* 
* ==> coredns [3f27e9ac11c1] <==
* [WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [c91557be79d1] <==
* [WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[ERROR] plugin/errors: 2 6969831528211158672.522469048498775381. HINFO: read udp 172.17.0.5:33113->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 6969831528211158672.522469048498775381. HINFO: read udp 172.17.0.5:46557->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 6969831528211158672.522469048498775381. HINFO: read udp 172.17.0.5:46019->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 6969831528211158672.522469048498775381. HINFO: read udp 172.17.0.5:60574->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_04_19T09_39_45_0700
                    minikube.k8s.io/version=v1.25.2
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 19 Apr 2022 07:39:41 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 17 Sep 2022 22:04:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 17 Sep 2022 22:04:21 +0000   Fri, 16 Sep 2022 08:43:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 17 Sep 2022 22:04:21 +0000   Fri, 16 Sep 2022 08:43:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 17 Sep 2022 22:04:21 +0000   Fri, 16 Sep 2022 08:43:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 17 Sep 2022 22:04:21 +0000   Fri, 16 Sep 2022 08:43:28 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4280692Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4280692Ki
  pods:               110
System Info:
  Machine ID:                 b6a262faae404a5db719705fd34b5c8b
  System UUID:                95a291a1-421a-4563-a82a-02b0ee342fdc
  Boot ID:                    41d7a136-3d7c-4622-8c22-927083312bd4
  Kernel Version:             5.10.47-linuxkit
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (29 in total)
  Namespace                   Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                            ------------  ----------  ---------------  -------------  ---
  default                     client-759d4b4cc-tvqpc                          500m (12%!)(MISSING)    1 (25%!)(MISSING)     512Mi (12%!)(MISSING)      1Gi (24%!)(MISSING)      150d
  default                     client-86d47f8bbd-s2pbv                         500m (12%!)(MISSING)    1 (25%!)(MISSING)     50Mi (1%!)(MISSING)        50Mi (1%!)(MISSING)      150d
  default                     client-mysql-564c9656d8-vlh7d                   500m (12%!)(MISSING)    1 (25%!)(MISSING)     512Mi (12%!)(MISSING)      1Gi (24%!)(MISSING)      150d
  default                     facture-mysql-8494b94f84-zc8hp                  500m (12%!)(MISSING)    1 (25%!)(MISSING)     512Mi (12%!)(MISSING)      1Gi (24%!)(MISSING)      150d
  default                     gateway-5576544cd7-86b6s                        500m (12%!)(MISSING)    1 (25%!)(MISSING)     512Mi (12%!)(MISSING)      1Gi (24%!)(MISSING)      150d
  default                     gateway-68bd77465d-zljlh                        500m (12%!)(MISSING)    1 (25%!)(MISSING)     512Mi (12%!)(MISSING)      1Gi (24%!)(MISSING)      150d
  default                     hello-minikube-7bc9d7884c-4r7d7                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  default                     hello-minikube-7bc9d7884c-bxjvx                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  default                     hello-minikube-7bc9d7884c-rbmls                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  default                     jhipster-grafana-7f645cc88f-pvcvj               100m (2%!)(MISSING)     200m (5%!)(MISSING)   100Mi (2%!)(MISSING)       250Mi (5%!)(MISSING)     150d
  default                     jhipster-prometheus-operator-9c5799c55-554jq    100m (2%!)(MISSING)     200m (5%!)(MISSING)   50Mi (1%!)(MISSING)        100Mi (2%!)(MISSING)     150d
  default                     jhipster-registry-0                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         150d
  default                     jhipster-registry-1                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         150d
  default                     kafka-broker-7d67df9dd-999gs                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  default                     mariadb-auth-sts-0                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  default                     mariadb-auth-sts-1                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26m
  default                     mariadb-order-sts-0                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  default                     mariadb-order-sts-1                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26m
  default                     zookeeper-55b668879d-p25gx                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  kube-system                 coredns-64897985d-g8vpv                         100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     151d
  kube-system                 etcd-minikube                                   100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         151d
  kube-system                 kube-apiserver-minikube                         250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  kube-system                 kube-controller-manager-minikube                200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  kube-system                 kube-proxy-kljks                                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  kube-system                 kube-scheduler-minikube                         100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  kube-system                 storage-provisioner                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
  kubernetes-dashboard        dashboard-metrics-scraper-57d8d5b8b8-829bj      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         49s
  kubernetes-dashboard        kubernetes-dashboard-56b9bdfcc8-2sk42           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         49s
  kubernetes-dashboard        kubernetes-dashboard-ccd587f44-66cp8            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         151d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                3950m (98%!)(MISSING)   6400m (160%!)(MISSING)
  memory             2930Mi (70%!)(MISSING)  5690Mi (136%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type     Reason                                            Age                  From        Message
  ----     ------                                            ----                 ----        -------
  Normal   Starting                                          11m                  kube-proxy  
  Warning  listen tcp4 :32260: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Normal   Starting                                          38h                  kube-proxy  
  Warning  listen tcp4 :30219: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Normal   Starting                                          38h                  kube-proxy  
  Warning  listen tcp4 :30219: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  38h                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Normal   Starting                                          48s                  kube-proxy  
  Warning  listen tcp4 :30219: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Normal   Starting                                          35h                  kube-proxy  
  Warning  listen tcp4 :31393: bind: address already in use  35h                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  35h                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  35h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  35h                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  35h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Normal   Starting                                          26m                  kube-proxy  
  Warning  listen tcp4 :32260: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Normal   Starting                                          29h                  kube-proxy  
  Warning  listen tcp4 :32260: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  29h                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Normal   Starting                                          6m32s                kube-proxy  
  Warning  listen tcp4 :32260: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Normal   Starting                                          47m                  kube-proxy  
  Warning  listen tcp4 :32131: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :32260: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for default/jhipster-grafana:http" (:32260/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  47m                  kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Warning  listen tcp4 :31006: bind: address already in use  43s                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :30219: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:30219/tcp4), skipping it
  Warning  listen tcp4 :32131: bind: address already in use  10m                  kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:32131/tcp4), skipping it
  Normal   Starting                                          10m                  kube-proxy  
  Warning  listen tcp4 :31006: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for default/hello-minikube" (:31006/tcp4), skipping it
  Warning  listen tcp4 :30182: bind: address already in use  6m31s                kube-proxy  can't open port "nodePort for default/zookeeper-service:zookeeper-port" (:30182/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  26m                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Warning  listen tcp4 :31393: bind: address already in use  11m                  kube-proxy  can't open port "nodePort for default/gateway:http" (:31393/tcp4), skipping it
  Normal   Starting                                          38h                  kubelet     Starting kubelet.
  Normal   NodeAllocatableEnforced                           38h                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory                           38h (x8 over 38h)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID                              38h (x7 over 38h)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure                             38h (x8 over 38h)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory                           38h (x8 over 38h)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                                          38h                  kubelet     Starting kubelet.
  Normal   NodeHasNoDiskPressure                             38h (x8 over 38h)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced                           38h                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID                              38h (x7 over 38h)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeNotReady                                      37h                  kubelet     Node minikube status is now: NodeNotReady
  Warning  ContainerGCFailed                                 37h (x2 over 37h)    kubelet     rpc error: code = DeadlineExceeded desc = context deadline exceeded
  Normal   Starting                                          35h                  kubelet     Starting kubelet.
  Normal   NodeAllocatableEnforced                           35h                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure                             35h (x8 over 35h)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory                           35h (x8 over 35h)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID                              35h (x7 over 35h)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced                           29h                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure                             29h (x8 over 29h)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   Starting                                          29h                  kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory                           29h (x8 over 29h)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID                              29h (x7 over 29h)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                                          47m                  kubelet     Starting kubelet.
  Normal   NodeAllocatableEnforced                           47m                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID                              47m (x7 over 47m)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory                           47m (x8 over 47m)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure                             47m (x8 over 47m)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory                           27m (x8 over 27m)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure                             27m (x8 over 27m)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID                              27m (x7 over 27m)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced                           27m                  kubelet     Updated Node Allocatable limit across pods
  Normal   Starting                                          27m                  kubelet     Starting kubelet.
  Normal   NodeHasSufficientPID                              12m (x7 over 12m)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                                          12m                  kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory                           12m (x8 over 12m)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeAllocatableEnforced                           12m                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure                             12m (x8 over 12m)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced                           10m                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID                              10m (x7 over 10m)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure                             10m (x8 over 10m)    kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   Starting                                          10m                  kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory                           10m (x8 over 10m)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                                          7m5s                 kubelet     Starting kubelet.
  Normal   NodeHasSufficientPID                              7m4s (x7 over 7m5s)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory                           7m4s (x8 over 7m5s)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure                             7m4s (x8 over 7m5s)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced                           7m4s                 kubelet     Updated Node Allocatable limit across pods
  Normal   Starting                                          72s                  kubelet     Starting kubelet.
  Normal   NodeAllocatableEnforced                           71s                  kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID                              70s (x7 over 72s)    kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory                           70s (x8 over 72s)    kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure                             70s (x8 over 72s)    kubelet     Node minikube status is now: NodeHasNoDiskPressure

* 
* ==> dmesg <==
* [  +0.000002]  asm_exc_page_fault+0x1e/0x30
[  +0.000017] RIP: 0033:0x5646b8ca064e
[  +0.000003] Code: 49 c7 c2 ff ff ff ff 31 c0 0f 1f 00 48 8d 7a 20 4c 89 12 4c 8d 82 a0 00 00 00 48 83 ee 01 48 83 e7 f8 48 c7 42 08 00 00 00 00 <48> c7 42 10 00 00 00 00 48 c7 42 18 00 00 00 00 48 c7 82 90 00 00
[  +0.000001] RSP: 002b:00007ffd54c437b0 EFLAGS: 00010202
[  +0.000002] RAX: 0000000000000000 RBX: 0000000000020000 RCX: 0000000000000000
[  +0.000002] RDX: 00007f493f529ff0 RSI: 000000000001154b RDI: 00007f493f52a010
[  +0.000001] RBP: 00007ffd54c43830 R08: 00007f493f52a090 R09: 00007f493ebff010
[  +0.000001] R10: ffffffffffffffff R11: 0000000000000246 R12: 00007f498107d625
[  +0.000001] R13: 00005646bba6a1f0 R14: 0000000000000000 R15: 00007ffd54c43d50
[  +0.000703] Memory cgroup out of memory: Killed process 32698 (mysqld) total-vm:1242952kB, anon-rss:353404kB, file-rss:22052kB, shmem-rss:0kB, UID:999 pgtables:1020kB oom_score_adj:878
[Sep17 22:02] java invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=989
[  +0.000014] CPU: 0 PID: 34245 Comm: java Tainted: G           O    T 5.10.47-linuxkit #1
[  +0.000001] Hardware name:  BHYVE, BIOS 1.00 03/14/2014
[  +0.000009] Call Trace:
[  +0.000009]  dump_stack+0x6b/0x83
[  +0.000003]  dump_header+0x4a/0x1ea
[  +0.000002]  oom_kill_process+0x80/0xfb
[  +0.000002]  out_of_memory+0x262/0x28d
[  +0.000002]  mem_cgroup_out_of_memory+0x7e/0xb3
[  +0.000002]  try_charge+0x41a/0x529
[  +0.000002]  mem_cgroup_charge+0x107/0x187
[  +0.000003]  handle_mm_fault+0x795/0xc79
[  +0.000003]  do_user_addr_fault+0x25b/0x30c
[  +0.000002]  exc_page_fault+0x11a/0x139
[  +0.000002]  ? asm_exc_page_fault+0x8/0x30
[  +0.000002]  asm_exc_page_fault+0x1e/0x30
[  +0.000002] RIP: 0033:0x7f5c09acdf80
[  +0.000002] Code: 03 48 8d 05 36 1a 8f 00 8b 13 89 10 e9 6e ff ff ff 90 0f 1f 84 00 00 00 00 00 55 48 39 f7 48 89 e5 73 12 0f 1f 80 00 00 00 00 <c6> 07 00 48 01 d7 48 39 fe 77 f5 5d c3 90 66 90 55 45 0f b6 c9 48
[  +0.000001] RSP: 002b:00007f5c08d1b6f0 EFLAGS: 00010206
[  +0.000002] RAX: 0000000000001000 RBX: 00007f5c0403c118 RCX: 00007f5c0a54d916
[  +0.000000] RDX: 0000000000001000 RSI: 00000000f5550000 RDI: 00000000f2877000
[  +0.000001] RBP: 00007f5c08d1b6f0 R08: 00000000ffffffff R09: 0000000005550000
[  +0.000001] R10: 0000000000000032 R11: 0000000000000206 R12: 00000000f0000000
[  +0.000001] R13: 0000000005550000 R14: 0000000000000000 R15: 00000000f5550000
[  +0.000073] Memory cgroup out of memory: Killed process 34229 (java) total-vm:707548kB, anon-rss:50644kB, file-rss:11684kB, shmem-rss:0kB, UID:1000 pgtables:244kB oom_score_adj:989
[Sep17 22:04] VM Thread invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=1000
[  +0.000033] CPU: 2 PID: 40498 Comm: VM Thread Tainted: G           O    T 5.10.47-linuxkit #1
[  +0.000002] Hardware name:  BHYVE, BIOS 1.00 03/14/2014
[  +0.000051] Call Trace:
[  +0.000340]  dump_stack+0x6b/0x83
[  +0.000058]  dump_header+0x4a/0x1ea
[  +0.000086]  oom_kill_process+0x80/0xfb
[  +0.000050]  out_of_memory+0x262/0x28d
[  +0.000015]  mem_cgroup_out_of_memory+0x7e/0xb3
[  +0.000069]  try_charge+0x41a/0x529
[  +0.000022]  mem_cgroup_charge+0x107/0x187
[  +0.000045]  handle_mm_fault+0x795/0xc79
[  +0.000100]  do_user_addr_fault+0x25b/0x30c
[  +0.000028]  exc_page_fault+0x11a/0x139
[  +0.000072]  ? asm_exc_page_fault+0x8/0x30
[  +0.000003]  asm_exc_page_fault+0x1e/0x30
[  +0.000099] RIP: 0033:0x7f2bf0808f80
[  +0.000005] Code: 03 48 8d 05 36 1a 8f 00 8b 13 89 10 e9 6e ff ff ff 90 0f 1f 84 00 00 00 00 00 55 48 39 f7 48 89 e5 73 12 0f 1f 80 00 00 00 00 <c6> 07 00 48 01 d7 48 39 fe 77 f5 5d c3 90 66 90 55 45 0f b6 c9 48
[  +0.000002] RSP: 002b:00007f2bee4023e0 EFLAGS: 00010206
[  +0.000003] RAX: 0000000000001000 RBX: 00007f2be803c3c8 RCX: 00007f2bf128ba46
[  +0.000001] RDX: 0000000000001000 RSI: 00000000e1d30000 RDI: 00000000e1bad000
[  +0.000002] RBP: 00007f2bee4023e0 R08: 00000000ffffffff R09: 0000000000000000
[  +0.000002] R10: 0000000000000032 R11: 0000000000000206 R12: 0000000000470000
[  +0.000003] R13: 0000000000470000 R14: 00000000e18c0000 R15: 0000000000000000
[  +0.004374] Memory cgroup out of memory: Killed process 39918 (java) total-vm:2785264kB, anon-rss:152380kB, file-rss:11456kB, shmem-rss:0kB, UID:1000 pgtables:584kB oom_score_adj:1000

* 
* ==> etcd [22d29df32811] <==
* {"level":"warn","ts":"2022-09-17T21:57:47.414Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.114906ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/gateway\" ","response":"range_response_count:1 size:1209"}
{"level":"info","ts":"2022-09-17T21:57:47.414Z","caller":"traceutil/trace.go:171","msg":"trace[642454091] range","detail":"{range_begin:/registry/services/specs/default/gateway; range_end:; response_count:1; response_revision:37648; }","duration":"106.136167ms","start":"2022-09-17T21:57:47.308Z","end":"2022-09-17T21:57:47.414Z","steps":["trace[642454091] 'agreement among raft nodes before linearized reading'  (duration: 106.097457ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.482Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"203.264548ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/facture\" ","response":"range_response_count:1 size:1006"}
{"level":"info","ts":"2022-09-17T21:57:47.482Z","caller":"traceutil/trace.go:171","msg":"trace[2044931209] range","detail":"{range_begin:/registry/services/specs/default/facture; range_end:; response_count:1; response_revision:37648; }","duration":"203.403343ms","start":"2022-09-17T21:57:47.279Z","end":"2022-09-17T21:57:47.482Z","steps":["trace[2044931209] 'agreement among raft nodes before linearized reading'  (duration: 203.135653ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.482Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"174.436936ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kube-system/coredns\" ","response":"range_response_count:1 size:4016"}
{"level":"warn","ts":"2022-09-17T21:57:47.486Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"192.69087ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/order-database-service-read\" ","response":"range_response_count:1 size:1188"}
{"level":"info","ts":"2022-09-17T21:57:47.486Z","caller":"traceutil/trace.go:171","msg":"trace[1494963178] range","detail":"{range_begin:/registry/services/specs/default/order-database-service-read; range_end:; response_count:1; response_revision:37648; }","duration":"192.815384ms","start":"2022-09-17T21:57:47.293Z","end":"2022-09-17T21:57:47.486Z","steps":["trace[1494963178] 'agreement among raft nodes before linearized reading'  (duration: 192.667345ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.486Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"192.844992ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/order-service\" ","response":"range_response_count:1 size:947"}
{"level":"info","ts":"2022-09-17T21:57:47.486Z","caller":"traceutil/trace.go:171","msg":"trace[1778980283] range","detail":"{range_begin:/registry/services/specs/default/order-service; range_end:; response_count:1; response_revision:37648; }","duration":"192.864698ms","start":"2022-09-17T21:57:47.293Z","end":"2022-09-17T21:57:47.486Z","steps":["trace[1778980283] 'agreement among raft nodes before linearized reading'  (duration: 192.829002ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.486Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"192.941451ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kube-system/kube-dns\" ","response":"range_response_count:1 size:1221"}
{"level":"info","ts":"2022-09-17T21:57:47.486Z","caller":"traceutil/trace.go:171","msg":"trace[1545180131] range","detail":"{range_begin:/registry/services/specs/kube-system/kube-dns; range_end:; response_count:1; response_revision:37648; }","duration":"192.957624ms","start":"2022-09-17T21:57:47.293Z","end":"2022-09-17T21:57:47.486Z","steps":["trace[1545180131] 'agreement among raft nodes before linearized reading'  (duration: 192.92819ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.487Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"193.527484ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/jhipster-registry\" ","response":"range_response_count:1 size:1284"}
{"level":"info","ts":"2022-09-17T21:57:47.487Z","caller":"traceutil/trace.go:171","msg":"trace[806012758] range","detail":"{range_begin:/registry/services/specs/default/jhipster-registry; range_end:; response_count:1; response_revision:37648; }","duration":"193.559721ms","start":"2022-09-17T21:57:47.293Z","end":"2022-09-17T21:57:47.487Z","steps":["trace[806012758] 'agreement among raft nodes before linearized reading'  (duration: 193.514994ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.494Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"200.803389ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/zookeeper-55b668879d-p25gx\" ","response":"range_response_count:1 size:2792"}
{"level":"info","ts":"2022-09-17T21:57:47.494Z","caller":"traceutil/trace.go:171","msg":"trace[1708282943] range","detail":"{range_begin:/registry/pods/default/zookeeper-55b668879d-p25gx; range_end:; response_count:1; response_revision:37649; }","duration":"200.889537ms","start":"2022-09-17T21:57:47.293Z","end":"2022-09-17T21:57:47.494Z","steps":["trace[1708282943] 'agreement among raft nodes before linearized reading'  (duration: 200.714472ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.482Z","caller":"traceutil/trace.go:171","msg":"trace[928816379] range","detail":"{range_begin:/registry/deployments/kube-system/coredns; range_end:; response_count:1; response_revision:37648; }","duration":"174.481205ms","start":"2022-09-17T21:57:47.308Z","end":"2022-09-17T21:57:47.482Z","steps":["trace[928816379] 'agreement among raft nodes before linearized reading'  (duration: 174.355329ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.724Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.50511ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015796193670874 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/default/client-mysql-swf8q\" mod_revision:37483 > success:<request_put:<key:\"/registry/endpointslices/default/client-mysql-swf8q\" value_size:866 >> failure:<request_range:<key:\"/registry/endpointslices/default/client-mysql-swf8q\" > >>","response":"size:18"}
{"level":"info","ts":"2022-09-17T21:57:47.725Z","caller":"traceutil/trace.go:171","msg":"trace[1980749352] transaction","detail":"{read_only:false; response_revision:37650; number_of_response:1; }","duration":"202.830256ms","start":"2022-09-17T21:57:47.522Z","end":"2022-09-17T21:57:47.725Z","steps":["trace[1980749352] 'process raft request'  (duration: 92.918125ms)","trace[1980749352] 'compare'  (duration: 105.411623ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T21:57:47.725Z","caller":"traceutil/trace.go:171","msg":"trace[546774180] transaction","detail":"{read_only:false; response_revision:37651; number_of_response:1; }","duration":"121.237241ms","start":"2022-09-17T21:57:47.604Z","end":"2022-09-17T21:57:47.725Z","steps":["trace[546774180] 'process raft request'  (duration: 120.169067ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.785Z","caller":"traceutil/trace.go:171","msg":"trace[1957549681] transaction","detail":"{read_only:false; response_revision:37653; number_of_response:1; }","duration":"171.607924ms","start":"2022-09-17T21:57:47.614Z","end":"2022-09-17T21:57:47.785Z","steps":["trace[1957549681] 'process raft request'  (duration: 110.500572ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.788Z","caller":"traceutil/trace.go:171","msg":"trace[1565186146] transaction","detail":"{read_only:false; response_revision:37656; number_of_response:1; }","duration":"107.635645ms","start":"2022-09-17T21:57:47.680Z","end":"2022-09-17T21:57:47.788Z","steps":["trace[1565186146] 'process raft request'  (duration: 107.432803ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.789Z","caller":"traceutil/trace.go:171","msg":"trace[1086805108] transaction","detail":"{read_only:false; response_revision:37654; number_of_response:1; }","duration":"174.495253ms","start":"2022-09-17T21:57:47.614Z","end":"2022-09-17T21:57:47.789Z","steps":["trace[1086805108] 'process raft request'  (duration: 110.042492ms)","trace[1086805108] 'store kv pair into bolt db' {req_type:put; key:/registry/pods/default/zookeeper-55b668879d-p25gx; req_size:2841; } (duration: 61.540425ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T21:57:47.791Z","caller":"traceutil/trace.go:171","msg":"trace[420233032] transaction","detail":"{read_only:false; response_revision:37652; number_of_response:1; }","duration":"116.680077ms","start":"2022-09-17T21:57:47.609Z","end":"2022-09-17T21:57:47.725Z","steps":["trace[420233032] 'process raft request'  (duration: 115.445278ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.794Z","caller":"traceutil/trace.go:171","msg":"trace[1440827403] transaction","detail":"{read_only:false; response_revision:37655; number_of_response:1; }","duration":"113.79008ms","start":"2022-09-17T21:57:47.680Z","end":"2022-09-17T21:57:47.794Z","steps":["trace[1440827403] 'process raft request'  (duration: 106.112429ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.922Z","caller":"traceutil/trace.go:171","msg":"trace[1650732424] linearizableReadLoop","detail":"{readStateIndex:47108; appliedIndex:47105; }","duration":"104.750723ms","start":"2022-09-17T21:57:47.818Z","end":"2022-09-17T21:57:47.922Z","steps":["trace[1650732424] 'read index received'  (duration: 97.44193ms)","trace[1650732424] 'applied index is now lower than readState.Index'  (duration: 7.308333ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T21:57:47.923Z","caller":"traceutil/trace.go:171","msg":"trace[1759629097] transaction","detail":"{read_only:false; response_revision:37664; number_of_response:1; }","duration":"105.237518ms","start":"2022-09-17T21:57:47.818Z","end":"2022-09-17T21:57:47.923Z","steps":["trace[1759629097] 'process raft request'  (duration: 104.57798ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.923Z","caller":"traceutil/trace.go:171","msg":"trace[1148804618] transaction","detail":"{read_only:false; response_revision:37661; number_of_response:1; }","duration":"106.148079ms","start":"2022-09-17T21:57:47.817Z","end":"2022-09-17T21:57:47.923Z","steps":["trace[1148804618] 'process raft request'  (duration: 98.257387ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.923Z","caller":"traceutil/trace.go:171","msg":"trace[298174203] transaction","detail":"{read_only:false; response_revision:37662; number_of_response:1; }","duration":"106.187015ms","start":"2022-09-17T21:57:47.817Z","end":"2022-09-17T21:57:47.923Z","steps":["trace[298174203] 'process raft request'  (duration: 105.238669ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:47.923Z","caller":"traceutil/trace.go:171","msg":"trace[713837935] transaction","detail":"{read_only:false; response_revision:37663; number_of_response:1; }","duration":"105.775611ms","start":"2022-09-17T21:57:47.818Z","end":"2022-09-17T21:57:47.923Z","steps":["trace[713837935] 'process raft request'  (duration: 104.797386ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:47.924Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.865552ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/gateway-68bd77465d-zljlh\" ","response":"range_response_count:1 size:6836"}
{"level":"info","ts":"2022-09-17T21:57:47.924Z","caller":"traceutil/trace.go:171","msg":"trace[1382395444] range","detail":"{range_begin:/registry/pods/default/gateway-68bd77465d-zljlh; range_end:; response_count:1; response_revision:37667; }","duration":"107.896602ms","start":"2022-09-17T21:57:47.816Z","end":"2022-09-17T21:57:47.924Z","steps":["trace[1382395444] 'agreement among raft nodes before linearized reading'  (duration: 107.84551ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:55.887Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"127.306336ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-17T21:57:55.887Z","caller":"traceutil/trace.go:171","msg":"trace[327118510] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37788; }","duration":"127.682861ms","start":"2022-09-17T21:57:55.759Z","end":"2022-09-17T21:57:55.887Z","steps":["trace[327118510] 'agreement among raft nodes before linearized reading'  (duration: 26.517894ms)","trace[327118510] 'range keys from in-memory index tree'  (duration: 100.736957ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-17T21:57:56.348Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"152.669967ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015796193671060 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/default/facture-mysql-fnmqd\" mod_revision:37691 > success:<request_put:<key:\"/registry/endpointslices/default/facture-mysql-fnmqd\" value_size:1048 >> failure:<request_range:<key:\"/registry/endpointslices/default/facture-mysql-fnmqd\" > >>","response":"size:18"}
{"level":"info","ts":"2022-09-17T21:57:56.349Z","caller":"traceutil/trace.go:171","msg":"trace[1942181081] transaction","detail":"{read_only:false; response_revision:37792; number_of_response:1; }","duration":"224.858663ms","start":"2022-09-17T21:57:56.124Z","end":"2022-09-17T21:57:56.349Z","steps":["trace[1942181081] 'process raft request'  (duration: 71.491704ms)","trace[1942181081] 'compare'  (duration: 55.149802ms)","trace[1942181081] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/endpointslices/default/facture-mysql-fnmqd; req_size:1105; } (duration: 96.948228ms)"],"step_count":3}
{"level":"info","ts":"2022-09-17T21:57:56.358Z","caller":"traceutil/trace.go:171","msg":"trace[393377591] transaction","detail":"{read_only:false; response_revision:37793; number_of_response:1; }","duration":"233.884963ms","start":"2022-09-17T21:57:56.124Z","end":"2022-09-17T21:57:56.358Z","steps":["trace[393377591] 'process raft request'  (duration: 224.205666ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:56.360Z","caller":"traceutil/trace.go:171","msg":"trace[1795292208] linearizableReadLoop","detail":"{readStateIndex:47241; appliedIndex:47238; }","duration":"100.832104ms","start":"2022-09-17T21:57:56.259Z","end":"2022-09-17T21:57:56.360Z","steps":["trace[1795292208] 'read index received'  (duration: 8.120799ms)","trace[1795292208] 'applied index is now lower than readState.Index'  (duration: 92.710863ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T21:57:56.360Z","caller":"traceutil/trace.go:171","msg":"trace[451026366] transaction","detail":"{read_only:false; response_revision:37794; number_of_response:1; }","duration":"210.943417ms","start":"2022-09-17T21:57:56.149Z","end":"2022-09-17T21:57:56.360Z","steps":["trace[451026366] 'process raft request'  (duration: 209.021335ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:56.360Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.587615ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/jhipster-registry-1\" ","response":"range_response_count:1 size:5850"}
{"level":"info","ts":"2022-09-17T21:57:56.360Z","caller":"traceutil/trace.go:171","msg":"trace[1538966353] range","detail":"{range_begin:/registry/pods/default/jhipster-registry-1; range_end:; response_count:1; response_revision:37794; }","duration":"101.668326ms","start":"2022-09-17T21:57:56.259Z","end":"2022-09-17T21:57:56.360Z","steps":["trace[1538966353] 'agreement among raft nodes before linearized reading'  (duration: 100.972824ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:56.579Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"130.654633ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015796193671065 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/jhipster-registry-1\" mod_revision:37566 > success:<request_put:<key:\"/registry/pods/default/jhipster-registry-1\" value_size:5700 >> failure:<request_range:<key:\"/registry/pods/default/jhipster-registry-1\" > >>","response":"size:18"}
{"level":"info","ts":"2022-09-17T21:57:56.580Z","caller":"traceutil/trace.go:171","msg":"trace[1971297990] transaction","detail":"{read_only:false; response_revision:37796; number_of_response:1; }","duration":"206.789045ms","start":"2022-09-17T21:57:56.373Z","end":"2022-09-17T21:57:56.580Z","steps":["trace[1971297990] 'process raft request'  (duration: 75.674711ms)","trace[1971297990] 'compare'  (duration: 130.490295ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T21:57:56.580Z","caller":"traceutil/trace.go:171","msg":"trace[822942566] transaction","detail":"{read_only:false; response_revision:37797; number_of_response:1; }","duration":"178.927091ms","start":"2022-09-17T21:57:56.401Z","end":"2022-09-17T21:57:56.580Z","steps":["trace[822942566] 'process raft request'  (duration: 178.601669ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T21:57:56.894Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"212.759397ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015796193671070 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/jhipster-registry\" mod_revision:37619 > success:<request_put:<key:\"/registry/services/endpoints/default/jhipster-registry\" value_size:654 >> failure:<request_range:<key:\"/registry/services/endpoints/default/jhipster-registry\" > >>","response":"size:18"}
{"level":"info","ts":"2022-09-17T21:57:56.895Z","caller":"traceutil/trace.go:171","msg":"trace[712869725] transaction","detail":"{read_only:false; response_revision:37802; number_of_response:1; }","duration":"207.029629ms","start":"2022-09-17T21:57:56.688Z","end":"2022-09-17T21:57:56.895Z","steps":["trace[712869725] 'process raft request'  (duration: 206.734365ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:56.895Z","caller":"traceutil/trace.go:171","msg":"trace[1349848149] transaction","detail":"{read_only:false; response_revision:37803; number_of_response:1; }","duration":"128.739361ms","start":"2022-09-17T21:57:56.766Z","end":"2022-09-17T21:57:56.895Z","steps":["trace[1349848149] 'process raft request'  (duration: 128.448862ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T21:57:56.895Z","caller":"traceutil/trace.go:171","msg":"trace[1607012494] transaction","detail":"{read_only:false; response_revision:37801; number_of_response:1; }","duration":"242.905244ms","start":"2022-09-17T21:57:56.652Z","end":"2022-09-17T21:57:56.895Z","steps":["trace[1607012494] 'process raft request'  (duration: 29.28863ms)","trace[1607012494] 'compare'  (duration: 84.335242ms)","trace[1607012494] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/default/jhipster-registry; req_size:713; } (duration: 128.298604ms)"],"step_count":3}
{"level":"info","ts":"2022-09-17T21:57:56.895Z","caller":"traceutil/trace.go:171","msg":"trace[2081887997] linearizableReadLoop","detail":"{readStateIndex:47249; appliedIndex:47247; }","duration":"131.802931ms","start":"2022-09-17T21:57:56.763Z","end":"2022-09-17T21:57:56.895Z","steps":["trace[2081887997] 'read index received'  (duration: 84.969765ms)","trace[2081887997] 'applied index is now lower than readState.Index'  (duration: 46.831725ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-17T21:57:56.895Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"131.957538ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-17T21:57:56.895Z","caller":"traceutil/trace.go:171","msg":"trace[249585468] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37803; }","duration":"131.997505ms","start":"2022-09-17T21:57:56.763Z","end":"2022-09-17T21:57:56.895Z","steps":["trace[249585468] 'agreement among raft nodes before linearized reading'  (duration: 131.893711ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:00:58.669Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.274556ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:9"}
{"level":"info","ts":"2022-09-17T22:00:58.671Z","caller":"traceutil/trace.go:171","msg":"trace[974221485] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:38384; }","duration":"212.623291ms","start":"2022-09-17T22:00:58.458Z","end":"2022-09-17T22:00:58.671Z","steps":["trace[974221485] 'agreement among raft nodes before linearized reading'  (duration: 69.322298ms)","trace[974221485] 'count revisions from in-memory index tree'  (duration: 105.602263ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T22:02:11.012Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-09-17T22:02:11.015Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2022/09/17 22:02:11 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
WARNING: 2022/09/17 22:02:11 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-09-17T22:02:11.211Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-09-17T22:02:11.213Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-09-17T22:02:11.282Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-09-17T22:02:11.283Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [3ad8ba901080] <==
* {"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 17"}
{"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 17"}
{"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 18"}
{"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 18"}
{"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 18"}
{"level":"info","ts":"2022-09-17T22:03:25.220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 18"}
{"level":"info","ts":"2022-09-17T22:03:25.222Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-09-17T22:03:25.223Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-09-17T22:03:25.223Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-09-17T22:03:25.226Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-09-17T22:03:25.226Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-09-17T22:03:25.234Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-09-17T22:03:25.235Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-09-17T22:03:35.178Z","caller":"traceutil/trace.go:171","msg":"trace[1706401216] linearizableReadLoop","detail":"{readStateIndex:48089; appliedIndex:48089; }","duration":"194.157571ms","start":"2022-09-17T22:03:34.984Z","end":"2022-09-17T22:03:35.178Z","steps":["trace[1706401216] 'read index received'  (duration: 194.149965ms)","trace[1706401216] 'applied index is now lower than readState.Index'  (duration: 6.463Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-17T22:03:35.181Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"197.022055ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/jhipster-prometheus-operator-9c5799c55-554jq\" ","response":"range_response_count:1 size:3346"}
{"level":"info","ts":"2022-09-17T22:03:35.181Z","caller":"traceutil/trace.go:171","msg":"trace[1641822486] range","detail":"{range_begin:/registry/pods/default/jhipster-prometheus-operator-9c5799c55-554jq; range_end:; response_count:1; response_revision:38570; }","duration":"197.525931ms","start":"2022-09-17T22:03:34.984Z","end":"2022-09-17T22:03:35.181Z","steps":["trace[1641822486] 'agreement among raft nodes before linearized reading'  (duration: 194.398229ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:37.088Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"165.124498ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015796284498258 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/client-759d4b4cc-tvqpc.1715c4f9585d3d69\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/client-759d4b4cc-tvqpc.1715c4f9585d3d69\" value_size:572 lease:8128015796284497826 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2022-09-17T22:03:37.088Z","caller":"traceutil/trace.go:171","msg":"trace[1069279682] linearizableReadLoop","detail":"{readStateIndex:48101; appliedIndex:48100; }","duration":"103.747227ms","start":"2022-09-17T22:03:36.984Z","end":"2022-09-17T22:03:37.088Z","steps":["trace[1069279682] 'read index received'  (duration: 95.751022ms)","trace[1069279682] 'applied index is now lower than readState.Index'  (duration: 7.994872ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-17T22:03:37.088Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.900229ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-58549894f-zswr4\" ","response":"range_response_count:1 size:4001"}
{"level":"info","ts":"2022-09-17T22:03:37.088Z","caller":"traceutil/trace.go:171","msg":"trace[521689937] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-58549894f-zswr4; range_end:; response_count:1; response_revision:38582; }","duration":"103.925649ms","start":"2022-09-17T22:03:36.984Z","end":"2022-09-17T22:03:37.088Z","steps":["trace[521689937] 'agreement among raft nodes before linearized reading'  (duration: 103.835453ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T22:03:37.088Z","caller":"traceutil/trace.go:171","msg":"trace[209833422] transaction","detail":"{read_only:false; response_revision:38582; number_of_response:1; }","duration":"171.788954ms","start":"2022-09-17T22:03:36.916Z","end":"2022-09-17T22:03:37.088Z","steps":["trace[209833422] 'compare'  (duration: 164.985894ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T22:03:41.127Z","caller":"traceutil/trace.go:171","msg":"trace[276714071] transaction","detail":"{read_only:false; response_revision:38642; number_of_response:1; }","duration":"100.402958ms","start":"2022-09-17T22:03:41.027Z","end":"2022-09-17T22:03:41.127Z","steps":["trace[276714071] 'process raft request'  (duration: 97.821593ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.686Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"104.840851ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/jhipster-prometheus-operator-9c5799c55\" ","response":"range_response_count:1 size:2446"}
{"level":"info","ts":"2022-09-17T22:03:41.686Z","caller":"traceutil/trace.go:171","msg":"trace[2057208418] range","detail":"{range_begin:/registry/replicasets/default/jhipster-prometheus-operator-9c5799c55; range_end:; response_count:1; response_revision:38690; }","duration":"104.963954ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.686Z","steps":["trace[2057208418] 'agreement among raft nodes before linearized reading'  (duration: 104.528652ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.687Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.322396ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/gateway-5576544cd7\" ","response":"range_response_count:1 size:5307"}
{"level":"info","ts":"2022-09-17T22:03:41.687Z","caller":"traceutil/trace.go:171","msg":"trace[1164794795] range","detail":"{range_begin:/registry/replicasets/default/gateway-5576544cd7; range_end:; response_count:1; response_revision:38690; }","duration":"100.386708ms","start":"2022-09-17T22:03:41.587Z","end":"2022-09-17T22:03:41.687Z","steps":["trace[1164794795] 'agreement among raft nodes before linearized reading'  (duration: 100.101393ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.687Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.627778ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/kafka-broker-7d67df9dd\" ","response":"range_response_count:1 size:2421"}
{"level":"info","ts":"2022-09-17T22:03:41.687Z","caller":"traceutil/trace.go:171","msg":"trace[1641504739] range","detail":"{range_begin:/registry/replicasets/default/kafka-broker-7d67df9dd; range_end:; response_count:1; response_revision:38690; }","duration":"100.657598ms","start":"2022-09-17T22:03:41.587Z","end":"2022-09-17T22:03:41.687Z","steps":["trace[1641504739] 'agreement among raft nodes before linearized reading'  (duration: 100.599063ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.687Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.15674ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kube-system/coredns-64897985d\" ","response":"range_response_count:1 size:3723"}
{"level":"info","ts":"2022-09-17T22:03:41.688Z","caller":"traceutil/trace.go:171","msg":"trace[944172813] range","detail":"{range_begin:/registry/replicasets/kube-system/coredns-64897985d; range_end:; response_count:1; response_revision:38690; }","duration":"106.187942ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.688Z","steps":["trace[944172813] 'agreement among raft nodes before linearized reading'  (duration: 106.132977ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.688Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.347727ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/auth-deployment-79695487b7\" ","response":"range_response_count:1 size:2478"}
{"level":"info","ts":"2022-09-17T22:03:41.688Z","caller":"traceutil/trace.go:171","msg":"trace[391225053] range","detail":"{range_begin:/registry/replicasets/default/auth-deployment-79695487b7; range_end:; response_count:1; response_revision:38690; }","duration":"106.375992ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.688Z","steps":["trace[391225053] 'agreement among raft nodes before linearized reading'  (duration: 106.3252ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.688Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.545444ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/client-mysql-564c9656d8\" ","response":"range_response_count:1 size:2635"}
{"level":"info","ts":"2022-09-17T22:03:41.688Z","caller":"traceutil/trace.go:171","msg":"trace[1227416735] range","detail":"{range_begin:/registry/replicasets/default/client-mysql-564c9656d8; range_end:; response_count:1; response_revision:38690; }","duration":"106.577193ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.688Z","steps":["trace[1227416735] 'agreement among raft nodes before linearized reading'  (duration: 106.523582ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.688Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.728187ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/facture-5f4d89dfdf\" ","response":"range_response_count:1 size:5018"}
{"level":"info","ts":"2022-09-17T22:03:41.688Z","caller":"traceutil/trace.go:171","msg":"trace[150707463] range","detail":"{range_begin:/registry/replicasets/default/facture-5f4d89dfdf; range_end:; response_count:1; response_revision:38690; }","duration":"106.755338ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.688Z","steps":["trace[150707463] 'agreement among raft nodes before linearized reading'  (duration: 106.708423ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.688Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.989954ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kubernetes-dashboard/kubernetes-dashboard-ccd587f44\" ","response":"range_response_count:1 size:3182"}
{"level":"warn","ts":"2022-09-17T22:03:41.692Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"110.799034ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/kubernetes-dashboard/dashboard-metrics-scraper-fz65f\" ","response":"range_response_count:1 size:1097"}
{"level":"info","ts":"2022-09-17T22:03:41.692Z","caller":"traceutil/trace.go:171","msg":"trace[955404809] range","detail":"{range_begin:/registry/endpointslices/kubernetes-dashboard/dashboard-metrics-scraper-fz65f; range_end:; response_count:1; response_revision:38690; }","duration":"110.865895ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.692Z","steps":["trace[955404809] 'agreement among raft nodes before linearized reading'  (duration: 110.744239ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.710Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"128.710166ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/hello-minikube-7bc9d7884c\" ","response":"range_response_count:1 size:1848"}
{"level":"info","ts":"2022-09-17T22:03:41.710Z","caller":"traceutil/trace.go:171","msg":"trace[641726855] range","detail":"{range_begin:/registry/replicasets/default/hello-minikube-7bc9d7884c; range_end:; response_count:1; response_revision:38690; }","duration":"128.787332ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.710Z","steps":["trace[641726855] 'agreement among raft nodes before linearized reading'  (duration: 128.6714ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.710Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"128.940333ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kubernetes-dashboard/dashboard-metrics-scraper-58549894f\" ","response":"range_response_count:1 size:3101"}
{"level":"info","ts":"2022-09-17T22:03:41.710Z","caller":"traceutil/trace.go:171","msg":"trace[1726645217] range","detail":"{range_begin:/registry/replicasets/kubernetes-dashboard/dashboard-metrics-scraper-58549894f; range_end:; response_count:1; response_revision:38690; }","duration":"128.960993ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.710Z","steps":["trace[1726645217] 'agreement among raft nodes before linearized reading'  (duration: 128.918051ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.710Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"129.142738ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/zookeeper-55b668879d\" ","response":"range_response_count:1 size:1945"}
{"level":"info","ts":"2022-09-17T22:03:41.710Z","caller":"traceutil/trace.go:171","msg":"trace[342051117] range","detail":"{range_begin:/registry/replicasets/default/zookeeper-55b668879d; range_end:; response_count:1; response_revision:38690; }","duration":"129.161745ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.710Z","steps":["trace[342051117] 'agreement among raft nodes before linearized reading'  (duration: 129.078638ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T22:03:41.688Z","caller":"traceutil/trace.go:171","msg":"trace[546125538] range","detail":"{range_begin:/registry/replicasets/kubernetes-dashboard/kubernetes-dashboard-ccd587f44; range_end:; response_count:1; response_revision:38690; }","duration":"107.018933ms","start":"2022-09-17T22:03:41.581Z","end":"2022-09-17T22:03:41.688Z","steps":["trace[546125538] 'agreement among raft nodes before linearized reading'  (duration: 106.963519ms)"],"step_count":1}
{"level":"info","ts":"2022-09-17T22:03:41.784Z","caller":"traceutil/trace.go:171","msg":"trace[624204506] linearizableReadLoop","detail":"{readStateIndex:48213; appliedIndex:48213; }","duration":"101.038531ms","start":"2022-09-17T22:03:41.683Z","end":"2022-09-17T22:03:41.784Z","steps":["trace[624204506] 'read index received'  (duration: 101.030268ms)","trace[624204506] 'applied index is now lower than readState.Index'  (duration: 7.09Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-17T22:03:41.785Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"155.691424ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/facture-748779995d\" ","response":"range_response_count:1 size:5115"}
{"level":"info","ts":"2022-09-17T22:03:41.785Z","caller":"traceutil/trace.go:171","msg":"trace[919691315] range","detail":"{range_begin:/registry/replicasets/default/facture-748779995d; range_end:; response_count:1; response_revision:38690; }","duration":"155.753549ms","start":"2022-09-17T22:03:41.629Z","end":"2022-09-17T22:03:41.785Z","steps":["trace[919691315] 'agreement among raft nodes before linearized reading'  (duration: 154.585897ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.785Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"159.21617ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/ingress-nginx/ingress-nginx-controller\" ","response":"range_response_count:1 size:8535"}
{"level":"info","ts":"2022-09-17T22:03:41.785Z","caller":"traceutil/trace.go:171","msg":"trace[152608146] range","detail":"{range_begin:/registry/deployments/ingress-nginx/ingress-nginx-controller; range_end:; response_count:1; response_revision:38690; }","duration":"159.294766ms","start":"2022-09-17T22:03:41.626Z","end":"2022-09-17T22:03:41.785Z","steps":["trace[152608146] 'agreement among raft nodes before linearized reading'  (duration: 157.869893ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:03:41.887Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"169.374135ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/gateway\" ","response":"range_response_count:1 size:8368"}
{"level":"info","ts":"2022-09-17T22:03:41.887Z","caller":"traceutil/trace.go:171","msg":"trace[1532540363] range","detail":"{range_begin:/registry/deployments/default/gateway; range_end:; response_count:1; response_revision:38692; }","duration":"169.501419ms","start":"2022-09-17T22:03:41.718Z","end":"2022-09-17T22:03:41.887Z","steps":["trace[1532540363] 'agreement among raft nodes before linearized reading'  (duration: 75.382937ms)","trace[1532540363] 'range keys from bolt db'  (duration: 93.907666ms)"],"step_count":2}
{"level":"info","ts":"2022-09-17T22:04:14.966Z","caller":"traceutil/trace.go:171","msg":"trace[56772285] transaction","detail":"{read_only:false; response_revision:38947; number_of_response:1; }","duration":"100.023409ms","start":"2022-09-17T22:04:14.866Z","end":"2022-09-17T22:04:14.966Z","steps":["trace[56772285] 'store kv pair into bolt db' {req_type:put; key:/registry/pods/kube-system/coredns-64897985d-g8vpv; req_size:4535; } (duration: 26.953196ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:04:29.807Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"246.221664ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:500 size:345994"}
{"level":"info","ts":"2022-09-17T22:04:29.838Z","caller":"traceutil/trace.go:171","msg":"trace[544309100] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:500; response_revision:39003; }","duration":"277.975473ms","start":"2022-09-17T22:04:29.560Z","end":"2022-09-17T22:04:29.838Z","steps":["trace[544309100] 'range keys from bolt db'  (duration: 245.647704ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:04:30.100Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"235.935024ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/hello-minikube-7bc9d7884c-bxjvx.1715c38d74da5ffc\\000\" range_end:\"/registry/events0\" limit:500 revision:39003 ","response":"range_response_count:500 size:346754"}
{"level":"info","ts":"2022-09-17T22:04:30.100Z","caller":"traceutil/trace.go:171","msg":"trace[2081776179] range","detail":"{range_begin:/registry/events/default/hello-minikube-7bc9d7884c-bxjvx.1715c38d74da5ffc\u0000; range_end:/registry/events0; response_count:500; response_revision:39003; }","duration":"236.032308ms","start":"2022-09-17T22:04:29.864Z","end":"2022-09-17T22:04:30.100Z","steps":["trace[2081776179] 'range keys from bolt db'  (duration: 235.407478ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-17T22:04:30.359Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"201.742405ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/mariadb-order-sts-1.1715c48dfa687f6f\\000\" range_end:\"/registry/events0\" limit:500 revision:39003 ","response":"range_response_count:500 size:338446"}
{"level":"info","ts":"2022-09-17T22:04:30.359Z","caller":"traceutil/trace.go:171","msg":"trace[430613926] range","detail":"{range_begin:/registry/events/default/mariadb-order-sts-1.1715c48dfa687f6f\u0000; range_end:/registry/events0; response_count:500; response_revision:39003; }","duration":"201.863469ms","start":"2022-09-17T22:04:30.158Z","end":"2022-09-17T22:04:30.359Z","steps":["trace[430613926] 'range keys from bolt db'  (duration: 200.444434ms)"],"step_count":1}

* 
* ==> kernel <==
*  22:04:31 up 15 min,  0 users,  load average: 1.36, 5.20, 3.78
Linux minikube 5.10.47-linuxkit #1 SMP Sat Jul 3 21:51:47 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [2867bd0518d0] <==
* W0917 22:03:26.172203       1 genericapiserver.go:538] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0917 22:03:26.176674       1 genericapiserver.go:538] Skipping API apps/v1beta2 because it has no resources.
W0917 22:03:26.176717       1 genericapiserver.go:538] Skipping API apps/v1beta1 because it has no resources.
W0917 22:03:26.178670       1 genericapiserver.go:538] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0917 22:03:26.182722       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0917 22:03:26.182772       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0917 22:03:26.211159       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0917 22:03:27.314754       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0917 22:03:27.314758       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 22:03:27.315053       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0917 22:03:27.315216       1 secure_serving.go:266] Serving securely on [::]:8443
I0917 22:03:27.315379       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 22:03:27.320410       1 available_controller.go:491] Starting AvailableConditionController
I0917 22:03:27.320446       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0917 22:03:27.322415       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0917 22:03:27.322428       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0917 22:03:27.322462       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0917 22:03:27.322467       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0917 22:03:27.324185       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0917 22:03:27.325842       1 autoregister_controller.go:141] Starting autoregister controller
I0917 22:03:27.325879       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0917 22:03:27.326876       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0917 22:03:27.327182       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0917 22:03:27.330122       1 crd_finalizer.go:266] Starting CRDFinalizer
I0917 22:03:27.333365       1 controller.go:83] Starting OpenAPI AggregationController
I0917 22:03:27.333459       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0917 22:03:27.334381       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0917 22:03:27.334411       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I0917 22:03:27.388547       1 naming_controller.go:291] Starting NamingConditionController
I0917 22:03:27.388662       1 controller.go:85] Starting OpenAPI controller
I0917 22:03:27.388686       1 establishing_controller.go:76] Starting EstablishingController
I0917 22:03:27.388800       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0917 22:03:27.394708       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 22:03:27.404392       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0917 22:03:27.420871       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0917 22:03:27.422682       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0917 22:03:27.422885       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0917 22:03:27.426129       1 cache.go:39] Caches are synced for autoregister controller
I0917 22:03:27.434737       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0917 22:03:27.434813       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0917 22:03:27.477986       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0917 22:03:27.481365       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0917 22:03:27.717793       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0917 22:03:27.718090       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0917 22:03:28.315126       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0917 22:03:28.329163       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0917 22:03:28.337964       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I0917 22:03:28.913827       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0917 22:03:28.931998       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0917 22:03:28.986613       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0917 22:03:29.009029       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0917 22:03:29.016463       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0917 22:03:40.808296       1 controller.go:611] quota admission added evaluator for: endpoints
I0917 22:03:40.910319       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0917 22:03:40.992027       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I0917 22:04:30.461960       1 trace.go:205] Trace[1546055762]: "List etcd3" key:/events,resourceVersion:,resourceVersionMatch:,limit:500,continue: (17-Sep-2022 22:04:29.559) (total time: 900ms):
Trace[1546055762]: [900.414271ms] [900.414271ms] END
I0917 22:04:30.473539       1 trace.go:205] Trace[1365435267]: "List" url:/api/v1/events,user-agent:kubectl/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:50e7e5e5-89d0-47e2-8674-cb0278407c88,client:127.0.0.1,accept:application/json, */*,protocol:HTTP/2.0 (17-Sep-2022 22:04:29.559) (total time: 913ms):
Trace[1365435267]: ---"Listing from storage done" 902ms (22:04:30.462)
Trace[1365435267]: [913.554963ms] [913.554963ms] END

* 
* ==> kube-apiserver [6bebc73c58a5] <==
* W0917 22:02:11.024363       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.024479       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.024598       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.029172       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.029899       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.079795       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.085649       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.088931       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.089094       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.089139       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.089177       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.090081       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.090189       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092078       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092217       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092321       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092403       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092429       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092485       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092554       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.092512       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.097053       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.097497       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.097537       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100014       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100108       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100163       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100199       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100268       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100299       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100327       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100338       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100362       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100388       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100412       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100680       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.100775       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.101012       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.101455       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.101780       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.101906       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102056       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102100       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102142       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102184       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102229       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102286       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102333       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102407       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102445       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.102482       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103080       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103130       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103188       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103207       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103219       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103274       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103324       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.103356       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0917 22:02:11.104150       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-controller-manager [91503d3b686a] <==
* I0917 21:57:45.876349       1 controllermanager.go:605] Started "ephemeral-volume"
I0917 21:57:45.876892       1 controller.go:170] Starting ephemeral volume controller
I0917 21:57:45.876975       1 shared_informer.go:240] Waiting for caches to sync for ephemeral
I0917 21:57:45.899885       1 controllermanager.go:605] Started "serviceaccount"
I0917 21:57:45.902318       1 serviceaccounts_controller.go:117] Starting service account controller
I0917 21:57:45.902336       1 shared_informer.go:240] Waiting for caches to sync for service account
I0917 21:57:45.979200       1 shared_informer.go:240] Waiting for caches to sync for resource quota
I0917 21:57:46.008426       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0917 21:57:46.203457       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I0917 21:57:46.203967       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I0917 21:57:46.218891       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I0917 21:57:46.288637       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I0917 21:57:46.288999       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I0917 21:57:46.394088       1 shared_informer.go:247] Caches are synced for service account 
W0917 21:57:46.396945       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0917 21:57:46.404389       1 shared_informer.go:247] Caches are synced for node 
I0917 21:57:46.404434       1 range_allocator.go:173] Starting range CIDR allocator
I0917 21:57:46.404440       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0917 21:57:46.404471       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0917 21:57:46.409982       1 shared_informer.go:247] Caches are synced for namespace 
I0917 21:57:46.474109       1 shared_informer.go:247] Caches are synced for expand 
I0917 21:57:46.475954       1 shared_informer.go:247] Caches are synced for TTL 
I0917 21:57:46.488024       1 job_controller.go:453] enqueueing job default/jhipster-grafana-dashboard
I0917 21:57:46.497925       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0917 21:57:46.497957       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0917 21:57:46.504394       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0917 21:57:46.504653       1 shared_informer.go:247] Caches are synced for PV protection 
I0917 21:57:46.508224       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0917 21:57:46.515866       1 shared_informer.go:247] Caches are synced for cronjob 
I0917 21:57:46.582425       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0917 21:57:46.590623       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0917 21:57:46.690511       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0917 21:57:46.692535       1 shared_informer.go:247] Caches are synced for crt configmap 
I0917 21:57:46.709924       1 shared_informer.go:247] Caches are synced for HPA 
I0917 21:57:46.710001       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0917 21:57:46.710510       1 shared_informer.go:247] Caches are synced for endpoint 
I0917 21:57:46.735197       1 shared_informer.go:247] Caches are synced for GC 
I0917 21:57:46.735697       1 shared_informer.go:247] Caches are synced for attach detach 
I0917 21:57:46.782013       1 shared_informer.go:247] Caches are synced for ephemeral 
I0917 21:57:46.795702       1 shared_informer.go:247] Caches are synced for daemon sets 
I0917 21:57:46.801713       1 shared_informer.go:247] Caches are synced for deployment 
I0917 21:57:46.818472       1 shared_informer.go:247] Caches are synced for stateful set 
I0917 21:57:46.877883       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0917 21:57:46.878939       1 shared_informer.go:247] Caches are synced for PVC protection 
I0917 21:57:46.878960       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0917 21:57:46.879006       1 shared_informer.go:247] Caches are synced for persistent volume 
I0917 21:57:46.883171       1 shared_informer.go:247] Caches are synced for taint 
I0917 21:57:46.888948       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
I0917 21:57:46.883217       1 shared_informer.go:247] Caches are synced for disruption 
I0917 21:57:46.889355       1 disruption.go:371] Sending events to api server.
I0917 21:57:46.889619       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0917 21:57:46.883317       1 shared_informer.go:247] Caches are synced for job 
W0917 21:57:46.896036       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0917 21:57:46.896123       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I0917 21:57:46.904203       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0917 21:57:47.087424       1 shared_informer.go:247] Caches are synced for resource quota 
I0917 21:57:47.090311       1 shared_informer.go:247] Caches are synced for garbage collector 
I0917 21:57:47.095839       1 shared_informer.go:247] Caches are synced for garbage collector 
I0917 21:57:47.095852       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0917 21:57:47.106295       1 shared_informer.go:247] Caches are synced for resource quota 

* 
* ==> kube-controller-manager [d779867ef906] <==
* I0917 22:03:40.290026       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I0917 22:03:40.290271       1 shared_informer.go:240] Waiting for caches to sync for resource quota
I0917 22:03:40.291052       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I0917 22:03:40.292963       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
W0917 22:03:40.401289       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0917 22:03:40.402121       1 shared_informer.go:247] Caches are synced for TTL 
I0917 22:03:40.402814       1 shared_informer.go:247] Caches are synced for PV protection 
I0917 22:03:40.411901       1 shared_informer.go:247] Caches are synced for expand 
I0917 22:03:40.427876       1 shared_informer.go:247] Caches are synced for node 
I0917 22:03:40.427926       1 range_allocator.go:173] Starting range CIDR allocator
I0917 22:03:40.427931       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0917 22:03:40.427942       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0917 22:03:40.430558       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0917 22:03:40.495982       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0917 22:03:40.496573       1 shared_informer.go:247] Caches are synced for service account 
I0917 22:03:40.496635       1 shared_informer.go:247] Caches are synced for cronjob 
I0917 22:03:40.497143       1 shared_informer.go:247] Caches are synced for crt configmap 
I0917 22:03:40.497272       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0917 22:03:40.497306       1 job_controller.go:453] enqueueing job default/jhipster-grafana-dashboard
I0917 22:03:40.497316       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0917 22:03:40.497321       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0917 22:03:40.511099       1 shared_informer.go:247] Caches are synced for namespace 
I0917 22:03:40.511200       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0917 22:03:40.518253       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0917 22:03:40.521358       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0917 22:03:40.524935       1 shared_informer.go:247] Caches are synced for ephemeral 
I0917 22:03:40.525399       1 shared_informer.go:247] Caches are synced for attach detach 
I0917 22:03:40.526264       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0917 22:03:40.580926       1 shared_informer.go:247] Caches are synced for persistent volume 
I0917 22:03:40.582764       1 shared_informer.go:247] Caches are synced for taint 
I0917 22:03:40.583947       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
W0917 22:03:40.584075       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0917 22:03:40.584182       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I0917 22:03:40.591286       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0917 22:03:40.591631       1 shared_informer.go:247] Caches are synced for stateful set 
I0917 22:03:40.593223       1 shared_informer.go:247] Caches are synced for job 
I0917 22:03:40.597880       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0917 22:03:40.604153       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0917 22:03:40.606952       1 shared_informer.go:247] Caches are synced for daemon sets 
I0917 22:03:40.607014       1 shared_informer.go:247] Caches are synced for HPA 
I0917 22:03:40.607341       1 shared_informer.go:247] Caches are synced for endpoint 
I0917 22:03:40.620319       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0917 22:03:40.620474       1 shared_informer.go:247] Caches are synced for deployment 
I0917 22:03:40.622216       1 shared_informer.go:247] Caches are synced for GC 
I0917 22:03:40.622385       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0917 22:03:40.623701       1 shared_informer.go:247] Caches are synced for PVC protection 
I0917 22:03:40.685810       1 shared_informer.go:247] Caches are synced for disruption 
I0917 22:03:40.685871       1 disruption.go:371] Sending events to api server.
I0917 22:03:41.097902       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-57d8d5b8b8 to 1"
I0917 22:03:41.097954       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-56b9bdfcc8 to 1"
I0917 22:03:41.290474       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-57d8d5b8b8" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-57d8d5b8b8-829bj"
I0917 22:03:41.290524       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-56b9bdfcc8" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-56b9bdfcc8-2sk42"
I0917 22:03:41.410582       1 shared_informer.go:247] Caches are synced for resource quota 
I0917 22:03:41.426405       1 shared_informer.go:247] Caches are synced for garbage collector 
I0917 22:03:41.490672       1 shared_informer.go:247] Caches are synced for resource quota 
I0917 22:03:41.501837       1 shared_informer.go:247] Caches are synced for garbage collector 
I0917 22:03:41.501938       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0917 22:04:19.173152       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set dashboard-metrics-scraper-58549894f to 0"
I0917 22:04:19.192423       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-58549894f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: dashboard-metrics-scraper-58549894f-zswr4"
W0917 22:04:21.852313       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kubernetes-dashboard/dashboard-metrics-scraper", retrying. Error: EndpointSlice informer cache is out of date

* 
* ==> kube-proxy [1a76d49de483] <==
* I0917 22:03:40.019017       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0917 22:03:40.020374       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0917 22:03:40.021626       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0917 22:03:42.510574       1 server_others.go:206] "Using iptables Proxier"
I0917 22:03:42.510647       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0917 22:03:42.510657       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0917 22:03:42.511528       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0917 22:03:42.582039       1 server.go:656] "Version info" version="v1.23.3"
I0917 22:03:42.602677       1 config.go:317] "Starting service config controller"
I0917 22:03:42.605622       1 shared_informer.go:240] Waiting for caches to sync for service config
I0917 22:03:42.617426       1 config.go:226] "Starting endpoint slice config controller"
I0917 22:03:42.617729       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0917 22:03:42.806928       1 shared_informer.go:247] Caches are synced for service config 
I0917 22:03:42.819456       1 shared_informer.go:247] Caches are synced for endpoint slice config 
E0917 22:03:47.607627       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31393: bind: address already in use" port={Description:nodePort for default/gateway:http IP: IPFamily:4 Port:31393 Protocol:TCP}
E0917 22:03:47.607699       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32131: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:http IP: IPFamily:4 Port:32131 Protocol:TCP}
E0917 22:03:47.607813       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32260: bind: address already in use" port={Description:nodePort for default/jhipster-grafana:http IP: IPFamily:4 Port:32260 Protocol:TCP}
E0917 22:03:47.607926       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30219: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:https IP: IPFamily:4 Port:30219 Protocol:TCP}
E0917 22:03:47.610700       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30182: bind: address already in use" port={Description:nodePort for default/zookeeper-service:zookeeper-port IP: IPFamily:4 Port:30182 Protocol:TCP}
E0917 22:03:47.610974       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31006: bind: address already in use" port={Description:nodePort for default/hello-minikube IP: IPFamily:4 Port:31006 Protocol:TCP}

* 
* ==> kube-proxy [32fb2b4ca27e] <==
* I0917 21:57:57.878624       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0917 21:57:57.878747       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0917 21:57:57.878780       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0917 21:57:58.483208       1 server_others.go:206] "Using iptables Proxier"
I0917 21:57:58.483248       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0917 21:57:58.483903       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0917 21:57:58.484516       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0917 21:57:58.550921       1 server.go:656] "Version info" version="v1.23.3"
I0917 21:57:58.563182       1 config.go:226] "Starting endpoint slice config controller"
I0917 21:57:58.564995       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0917 21:57:58.565470       1 config.go:317] "Starting service config controller"
I0917 21:57:58.567525       1 shared_informer.go:240] Waiting for caches to sync for service config
I0917 21:57:58.666260       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I0917 21:57:58.668678       1 shared_informer.go:247] Caches are synced for service config 
E0917 21:57:59.268653       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32260: bind: address already in use" port={Description:nodePort for default/jhipster-grafana:http IP: IPFamily:4 Port:32260 Protocol:TCP}
E0917 21:57:59.268827       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30182: bind: address already in use" port={Description:nodePort for default/zookeeper-service:zookeeper-port IP: IPFamily:4 Port:30182 Protocol:TCP}
E0917 21:57:59.268891       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32131: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:http IP: IPFamily:4 Port:32131 Protocol:TCP}
E0917 21:57:59.269032       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31393: bind: address already in use" port={Description:nodePort for default/gateway:http IP: IPFamily:4 Port:31393 Protocol:TCP}
E0917 21:57:59.269163       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31006: bind: address already in use" port={Description:nodePort for default/hello-minikube IP: IPFamily:4 Port:31006 Protocol:TCP}
E0917 21:57:59.269322       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30219: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:https IP: IPFamily:4 Port:30219 Protocol:TCP}

* 
* ==> kube-scheduler [202ed50db941] <==
* I0917 22:03:23.718917       1 serving.go:348] Generated self-signed cert in-memory
I0917 22:03:27.425437       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.3"
I0917 22:03:27.497533       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0917 22:03:27.497712       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I0917 22:03:27.497879       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0917 22:03:27.497917       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0917 22:03:27.498506       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0917 22:03:27.498683       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 22:03:27.498800       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 22:03:27.498696       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 22:03:27.598214       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
I0917 22:03:27.598972       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0917 22:03:27.599408       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 

* 
* ==> kube-scheduler [5177b031a40a] <==
* I0917 21:57:28.643598       1 serving.go:348] Generated self-signed cert in-memory
W0917 21:57:32.421427       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0917 21:57:32.421546       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
W0917 21:57:32.421573       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0917 21:57:32.421708       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0917 21:57:32.498002       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.3"
I0917 21:57:32.514779       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0917 21:57:32.515864       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 21:57:32.515950       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 21:57:32.519044       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 21:57:32.616813       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0917 22:02:10.495719       1 configmap_cafile_content.go:222] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 22:02:10.497179       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0917 22:02:10.497280       1 secure_serving.go:311] Stopped listening on 127.0.0.1:10259

* 
* ==> kubelet <==
* -- Logs begin at Sat 2022-09-17 22:02:57 UTC, end at Sat 2022-09-17 22:04:35 UTC. --
Sep 17 22:03:58 minikube kubelet[951]: I0917 22:03:58.597180     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-order-sts-0 through plugin: invalid network status for"
Sep 17 22:03:58 minikube kubelet[951]: I0917 22:03:58.674483     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-auth-sts-1 through plugin: invalid network status for"
Sep 17 22:03:58 minikube kubelet[951]: I0917 22:03:58.698665     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kubernetes-dashboard/kubernetes-dashboard-ccd587f44-66cp8 through plugin: invalid network status for"
Sep 17 22:03:58 minikube kubelet[951]: I0917 22:03:58.781201     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/gateway-5576544cd7-86b6s through plugin: invalid network status for"
Sep 17 22:03:58 minikube kubelet[951]: I0917 22:03:58.806803     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-registry-0 through plugin: invalid network status for"
Sep 17 22:03:59 minikube kubelet[951]: I0917 22:03:59.990435     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-auth-sts-1 through plugin: invalid network status for"
Sep 17 22:04:00 minikube kubelet[951]: I0917 22:04:00.092733     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-auth-sts-0 through plugin: invalid network status for"
Sep 17 22:04:01 minikube kubelet[951]: I0917 22:04:01.382124     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-order-sts-1 through plugin: invalid network status for"
Sep 17 22:04:02 minikube kubelet[951]: I0917 22:04:02.861572     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-order-sts-1 through plugin: invalid network status for"
Sep 17 22:04:03 minikube kubelet[951]: I0917 22:04:03.003458     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-order-sts-0 through plugin: invalid network status for"
Sep 17 22:04:04 minikube kubelet[951]: I0917 22:04:04.278641     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/mariadb-order-sts-0 through plugin: invalid network status for"
Sep 17 22:04:06 minikube kubelet[951]: E0917 22:04:06.303158     951 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Sep 17 22:04:06 minikube kubelet[951]: E0917 22:04:06.303239     951 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Sep 17 22:04:12 minikube kubelet[951]: I0917 22:04:12.876779     951 scope.go:110] "RemoveContainer" containerID="d5c1a70b8b0f7fcb8ae6f255b1528c10f29c27331cfce2973b238813b6b04837"
Sep 17 22:04:12 minikube kubelet[951]: I0917 22:04:12.878055     951 scope.go:110] "RemoveContainer" containerID="15b63c33e3547add8889e11c63a0e59157aa583405da0c2f3003503dae65a058"
Sep 17 22:04:12 minikube kubelet[951]: E0917 22:04:12.878698     951 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(abc3491c-5e34-4d70-8505-0a1d6285582e)\"" pod="kube-system/storage-provisioner" podUID=abc3491c-5e34-4d70-8505-0a1d6285582e
Sep 17 22:04:16 minikube kubelet[951]: E0917 22:04:16.496355     951 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Sep 17 22:04:16 minikube kubelet[951]: E0917 22:04:16.503242     951 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.516377     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/client-86d47f8bbd-s2pbv through plugin: invalid network status for"
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.573393     951 scope.go:110] "RemoveContainer" containerID="6201301ac7c18646c7e8ac5a83013d398f8f9019c0a07713c2a2a3ddb7a0eb72"
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.587620     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kubernetes-dashboard/dashboard-metrics-scraper-57d8d5b8b8-829bj through plugin: invalid network status for"
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.700983     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-prometheus-operator-9c5799c55-554jq through plugin: invalid network status for"
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.762077     951 scope.go:110] "RemoveContainer" containerID="a1d9823d67b24a784b7faac4f515e52274e00ce3f71f9bce7be3658f3be3427a"
Sep 17 22:04:17 minikube kubelet[951]: I0917 22:04:17.762787     951 scope.go:110] "RemoveContainer" containerID="26c61b45899c030eb72d16b927ff8541da1c41512d3e704a2d15d0912c5f7241"
Sep 17 22:04:17 minikube kubelet[951]: E0917 22:04:17.763150     951 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"prometheus-operator\" with CrashLoopBackOff: \"back-off 10s restarting failed container=prometheus-operator pod=jhipster-prometheus-operator-9c5799c55-554jq_default(e18d03a8-6544-43cb-a598-ada55cfbaa1e)\"" pod="default/jhipster-prometheus-operator-9c5799c55-554jq" podUID=e18d03a8-6544-43cb-a598-ada55cfbaa1e
Sep 17 22:04:18 minikube kubelet[951]: I0917 22:04:18.804888     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-prometheus-operator-9c5799c55-554jq through plugin: invalid network status for"
Sep 17 22:04:18 minikube kubelet[951]: I0917 22:04:18.860577     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-registry-1 through plugin: invalid network status for"
Sep 17 22:04:18 minikube kubelet[951]: I0917 22:04:18.883665     951 scope.go:110] "RemoveContainer" containerID="a75fb3fd2e9868cfb35349e3a94fff7a1d1c1565171a913793c11cab096c9b5f"
Sep 17 22:04:18 minikube kubelet[951]: I0917 22:04:18.890491     951 scope.go:110] "RemoveContainer" containerID="96a597c46b3122bf8697ef7f0766bf817ce660791e1b1ae56abb41d8a6f9aa6d"
Sep 17 22:04:18 minikube kubelet[951]: E0917 22:04:18.891077     951 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jhipster-registry\" with CrashLoopBackOff: \"back-off 10s restarting failed container=jhipster-registry pod=jhipster-registry-1_default(10dba221-6075-4733-b4f7-7997fc47e9d4)\"" pod="default/jhipster-registry-1" podUID=10dba221-6075-4733-b4f7-7997fc47e9d4
Sep 17 22:04:18 minikube kubelet[951]: I0917 22:04:18.892672     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kubernetes-dashboard/dashboard-metrics-scraper-57d8d5b8b8-829bj through plugin: invalid network status for"
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.264697     951 scope.go:110] "RemoveContainer" containerID="3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe"
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.343357     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-registry-1 through plugin: invalid network status for"
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.451145     951 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/4d12a3e7-46f5-48f5-9934-841d10002ad7-tmp-volume\") pod \"4d12a3e7-46f5-48f5-9934-841d10002ad7\" (UID: \"4d12a3e7-46f5-48f5-9934-841d10002ad7\") "
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.451355     951 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kckpk\" (UniqueName: \"kubernetes.io/projected/4d12a3e7-46f5-48f5-9934-841d10002ad7-kube-api-access-kckpk\") pod \"4d12a3e7-46f5-48f5-9934-841d10002ad7\" (UID: \"4d12a3e7-46f5-48f5-9934-841d10002ad7\") "
Sep 17 22:04:20 minikube kubelet[951]: W0917 22:04:20.462148     951 empty_dir.go:517] Warning: Failed to clear quota on /var/lib/kubelet/pods/4d12a3e7-46f5-48f5-9934-841d10002ad7/volumes/kubernetes.io~empty-dir/tmp-volume: clearQuota called, but quotas disabled
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.475887     951 scope.go:110] "RemoveContainer" containerID="3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe"
Sep 17 22:04:20 minikube kubelet[951]: E0917 22:04:20.486124     951 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe" containerID="3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe"
Sep 17 22:04:20 minikube kubelet[951]: E0917 22:04:20.537539     951 kuberuntime_gc.go:146] "Failed to remove container" err="failed to get container status \"3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe\": rpc error: code = Unknown desc = Error: No such container: 3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe" containerID="3c0881bc1c05a8fff74b1dc99fe3b9740ac2d9e36feec09c63e20957e0eec6fe"
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.537678     951 scope.go:110] "RemoveContainer" containerID="8282a462cc3d754b38108bafa008282defb567e3b24c5c1e1b9e58ede4b8a5f7"
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.555402     951 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4d12a3e7-46f5-48f5-9934-841d10002ad7-tmp-volume" (OuterVolumeSpecName: "tmp-volume") pod "4d12a3e7-46f5-48f5-9934-841d10002ad7" (UID: "4d12a3e7-46f5-48f5-9934-841d10002ad7"). InnerVolumeSpecName "tmp-volume". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.572675     951 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4d12a3e7-46f5-48f5-9934-841d10002ad7-kube-api-access-kckpk" (OuterVolumeSpecName: "kube-api-access-kckpk") pod "4d12a3e7-46f5-48f5-9934-841d10002ad7" (UID: "4d12a3e7-46f5-48f5-9934-841d10002ad7"). InnerVolumeSpecName "kube-api-access-kckpk". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.654120     951 reconciler.go:300] "Volume detached for volume \"kube-api-access-kckpk\" (UniqueName: \"kubernetes.io/projected/4d12a3e7-46f5-48f5-9934-841d10002ad7-kube-api-access-kckpk\") on node \"minikube\" DevicePath \"\""
Sep 17 22:04:20 minikube kubelet[951]: I0917 22:04:20.654158     951 reconciler.go:300] "Volume detached for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/4d12a3e7-46f5-48f5-9934-841d10002ad7-tmp-volume\") on node \"minikube\" DevicePath \"\""
Sep 17 22:04:22 minikube kubelet[951]: I0917 22:04:22.790156     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-registry-1 through plugin: invalid network status for"
Sep 17 22:04:22 minikube kubelet[951]: I0917 22:04:22.845373     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/gateway-5576544cd7-86b6s through plugin: invalid network status for"
Sep 17 22:04:22 minikube kubelet[951]: I0917 22:04:22.865500     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-prometheus-operator-9c5799c55-554jq through plugin: invalid network status for"
Sep 17 22:04:22 minikube kubelet[951]: I0917 22:04:22.939193     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/gateway-68bd77465d-zljlh through plugin: invalid network status for"
Sep 17 22:04:22 minikube kubelet[951]: I0917 22:04:22.990932     951 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=4d12a3e7-46f5-48f5-9934-841d10002ad7 path="/var/lib/kubelet/pods/4d12a3e7-46f5-48f5-9934-841d10002ad7/volumes"
Sep 17 22:04:25 minikube kubelet[951]: I0917 22:04:25.162563     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/client-759d4b4cc-tvqpc through plugin: invalid network status for"
Sep 17 22:04:25 minikube kubelet[951]: E0917 22:04:25.239962     951 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with CrashLoopBackOff: \"back-off 10s restarting failed container=init-ds pod=client-759d4b4cc-tvqpc_default(7031ffa6-6769-4d15-b59b-30d6a42dc005)\"" pod="default/client-759d4b4cc-tvqpc" podUID=7031ffa6-6769-4d15-b59b-30d6a42dc005
Sep 17 22:04:26 minikube kubelet[951]: I0917 22:04:26.876154     951 scope.go:110] "RemoveContainer" containerID="15b63c33e3547add8889e11c63a0e59157aa583405da0c2f3003503dae65a058"
Sep 17 22:04:32 minikube kubelet[951]: I0917 22:04:32.876833     951 scope.go:110] "RemoveContainer" containerID="26c61b45899c030eb72d16b927ff8541da1c41512d3e704a2d15d0912c5f7241"
Sep 17 22:04:32 minikube kubelet[951]: I0917 22:04:32.877795     951 scope.go:110] "RemoveContainer" containerID="96a597c46b3122bf8697ef7f0766bf817ce660791e1b1ae56abb41d8a6f9aa6d"
Sep 17 22:04:33 minikube kubelet[951]: I0917 22:04:33.664442     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-prometheus-operator-9c5799c55-554jq through plugin: invalid network status for"
Sep 17 22:04:34 minikube kubelet[951]: I0917 22:04:34.342962     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/kafka-broker-7d67df9dd-999gs through plugin: invalid network status for"
Sep 17 22:04:34 minikube kubelet[951]: I0917 22:04:34.469430     951 scope.go:110] "RemoveContainer" containerID="5f731b3df75ceda8bf518a80b8e141f7b54d7e65b25077cc9a736595aeabd66a"
Sep 17 22:04:34 minikube kubelet[951]: I0917 22:04:34.469968     951 scope.go:110] "RemoveContainer" containerID="a78a2e7d9b5189677a80b5a71bc12394b2a8ef9b3a79b344398798b8cc45e208"
Sep 17 22:04:34 minikube kubelet[951]: E0917 22:04:34.479507     951 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kafka-broker\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kafka-broker pod=kafka-broker-7d67df9dd-999gs_default(90da7c58-89af-4f60-87a4-860de9003a26)\"" pod="default/kafka-broker-7d67df9dd-999gs" podUID=90da7c58-89af-4f60-87a4-860de9003a26
Sep 17 22:04:34 minikube kubelet[951]: I0917 22:04:34.561276     951 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/jhipster-registry-1 through plugin: invalid network status for"

* 
* ==> kubernetes-dashboard [03132c436f49] <==
* 2022/09/17 22:03:51 Using namespace: kubernetes-dashboard
2022/09/17 22:03:51 Using in-cluster config to connect to apiserver
2022/09/17 22:03:51 Using secret token for csrf signing
2022/09/17 22:03:51 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/09/17 22:03:51 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2022/09/17 22:03:51 Successful initial request to the apiserver, version: v1.23.3
2022/09/17 22:03:51 Generating JWE encryption key
2022/09/17 22:03:51 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/09/17 22:03:51 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/09/17 22:03:55 Initializing JWE encryption key from synchronized object
2022/09/17 22:03:55 Creating in-cluster Sidecar client
2022/09/17 22:03:55 Serving insecurely on HTTP port: 9090
2022/09/17 22:03:55 Successful request to sidecar
2022/09/17 22:03:51 Starting overwatch

* 
* ==> kubernetes-dashboard [bda219223c48] <==
* 2022/09/17 21:58:34 Using namespace: kubernetes-dashboard
2022/09/17 21:58:34 Using in-cluster config to connect to apiserver
2022/09/17 21:58:34 Using secret token for csrf signing
2022/09/17 21:58:34 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/09/17 21:58:34 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2022/09/17 21:58:34 Successful initial request to the apiserver, version: v1.23.3
2022/09/17 21:58:34 Generating JWE encryption key
2022/09/17 21:58:34 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/09/17 21:58:34 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/09/17 21:58:34 Initializing JWE encryption key from synchronized object
2022/09/17 21:58:34 Creating in-cluster Sidecar client
2022/09/17 21:58:34 Successful request to sidecar
2022/09/17 21:58:34 Serving insecurely on HTTP port: 9090
2022/09/17 21:58:34 Starting overwatch

* 
* ==> storage-provisioner [15b63c33e354] <==
* I0917 22:03:42.200672       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0917 22:04:12.263255       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

